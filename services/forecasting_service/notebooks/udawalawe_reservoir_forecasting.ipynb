{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸŒŠ Udawalawe Reservoir Forecasting Model\n",
    "## AI-Driven Probabilistic Forecasting for Water Management\n",
    "---\n",
    "**Objective:** Develop ML models to forecast inflow, storage level, and irrigation demand (1-14 day horizon)\n",
    "\n",
    "**Models Implemented:**\n",
    "- ARIMA (Time-Series Baseline)\n",
    "- Random Forest / Gradient Boosting (ML)\n",
    "- LSTM / GRU (Deep Learning)\n",
    "- Quantile Regression (Probabilistic Outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "All libraries imported successfully!\n",
      "TensorFlow version: 2.20.0\n"
     ]
    }
   ],
   "source": [
    "# Install required packages if not already installed\n",
    "%pip install -q tensorflow statsmodels\n",
    "\n",
    "# Core Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# ML Models\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "# Time Series\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "print('All libraries imported successfully!')\n",
    "print(f'TensorFlow version: {tf.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Download CHIRPS + NASA POWER Data\n",
    "\n",
    "This section downloads remotely-sensed meteorological and precipitation data:\n",
    "- **CHIRPS**: Climate Hazards Group InfraRed Precipitation with Stations (via ClimateSERV API)\n",
    "- **NASA POWER**: Prediction Of Worldwide Energy Resource (daily meteorology)\n",
    "- **ETâ‚€**: Reference Evapotranspiration (computed from meteorological data using Hargreaves method)\n",
    "\n",
    "**Study Area**: Udawalawe Reservoir, Sri Lanka (~6.5Â°N, 80.75Â°E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Data downloader module loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Import the data downloader module\n",
    "import sys\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "from data_downloader import (\n",
    "    download_and_integrate, \n",
    "    CHIRPSDownloader, \n",
    "    NASAPOWERDownloader,\n",
    "    DataIntegrator\n",
    ")\n",
    "\n",
    "print(\"âœ“ Data downloader module loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download Configuration:\n",
      "  Location: Udawalawe Reservoir (6.5Â°N, 80.75Â°E)\n",
      "  Elevation: 380m\n",
      "  Date Range: 2020-01-01 to 2023-12-31\n",
      "  Duration: 1460 days\n"
     ]
    }
   ],
   "source": [
    "# Configuration: Udawalawe Reservoir coordinates\n",
    "LAT = 6.5                              # Latitude (Â°N)\n",
    "LON = 80.75                            # Longitude (Â°E)\n",
    "ELEVATION = 380                        # Elevation (m above sea level)\n",
    "\n",
    "# Date range for data download\n",
    "# NOTE: Adjust these dates based on your hydrological data availability\n",
    "START_DATE = \"2020-01-01\"\n",
    "END_DATE = \"2023-12-31\"\n",
    "\n",
    "print(f\"Download Configuration:\")\n",
    "print(f\"  Location: Udawalawe Reservoir ({LAT}Â°N, {LON}Â°E)\")\n",
    "print(f\"  Elevation: {ELEVATION}m\")\n",
    "print(f\"  Date Range: {START_DATE} to {END_DATE}\")\n",
    "print(f\"  Duration: {(pd.to_datetime(END_DATE) - pd.to_datetime(START_DATE)).days} days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-03 14:37:51,811 - INFO - ============================================================\n",
      "2026-01-03 14:37:51,812 - INFO - STARTING INTEGRATED DATA DOWNLOAD\n",
      "2026-01-03 14:37:51,813 - INFO - ============================================================\n",
      "2026-01-03 14:37:51,815 - INFO - Downloading NASA POWER data (2020-01-01 to 2023-12-31)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "DOWNLOADING METEOROLOGICAL DATA FROM REMOTE SOURCES\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-03 14:37:52,978 - ERROR - NASA POWER download failed: 404 Client Error: Not Found for url: https://power.larc.nasa.gov/api/v1/daily?start=20200101&end=20231231&latitude=6.5&longitude=80.75&parameters=T2M%2CT2M_MAX%2CT2M_MIN%2CRH2M%2CWS2M%2CALLSKY_SFC_SW_DWN&community=ag&format=json\n",
      "2026-01-03 14:37:52,990 - INFO - Generating synthetic CHIRPS data (for testing)...\n",
      "2026-01-03 14:37:53,029 - INFO - âœ“ Generated 48 synthetic CHIRPS records\n",
      "2026-01-03 14:37:53,031 - INFO - \n",
      "============================================================\n",
      "2026-01-03 14:37:53,032 - INFO - DATA SUMMARY\n",
      "2026-01-03 14:37:53,033 - INFO - ============================================================\n",
      "2026-01-03 14:37:53,035 - INFO - Date range: 2020-01-01 00:00:00 to 2023-12-01 00:00:00\n",
      "2026-01-03 14:37:53,036 - INFO - Records: 48\n",
      "2026-01-03 14:37:53,038 - INFO - \n",
      "Columns and Missing Values:\n",
      "2026-01-03 14:37:53,040 - INFO -   precipitation_mm: 0.0% missing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Downloaded 48 meteorological records\n",
      "\n",
      "First 5 rows:\n",
      "            precipitation_mm\n",
      "date                        \n",
      "2020-01-01          7.190419\n",
      "2020-02-01         49.750254\n",
      "2020-03-01         94.420505\n",
      "2020-04-01        120.114964\n",
      "2020-05-01        144.290669\n",
      "\n",
      "Data types and info:\n",
      "precipitation_mm    float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Download integrated meteorological and precipitation data\n",
    "# use_synthetic_chirps=True uses realistic synthetic data when ClimateSERV API is unavailable\n",
    "# Set to False to use live API calls (requires internet and may take longer)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DOWNLOADING METEOROLOGICAL DATA FROM REMOTE SOURCES\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Download with synthetic CHIRPS for testing (fast, no API calls)\n",
    "meteorological_df = download_and_integrate(\n",
    "    lat=LAT,\n",
    "    lon=LON,\n",
    "    start_date=START_DATE,\n",
    "    end_date=END_DATE,\n",
    "    elevation=ELEVATION,\n",
    "    use_synthetic_chirps=True  # Set to False to use live CHIRPS API\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Downloaded {len(meteorological_df)} meteorological records\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "print(meteorological_df.head())\n",
    "\n",
    "print(f\"\\nData types and info:\")\n",
    "print(meteorological_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load & Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: (365, 16)\n",
      "\n",
      "Columns: ['Date', 'Water_Level_(mMSL)', 'Water_Level_(ftMSL)', 'Total_Storage_(MCM)', 'Active_Storage_(MCM)', 'Gross_Storage_Percentage_(%)', 'Net_Percentage_(%)', 'Energy_(MWh)', 'LB_Main_Canal_(MCM)', 'RB_Main_Canal_(MCM)', 'Main_Canals:_LB+RB_(MCM)', 'Spillway_(MCM)', 'Evaporation_(mm)', 'Evaporation_(MCM)', 'Inflow_(MCM)', 'Rainfall_(mm)']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Water_Level_(mMSL)</th>\n",
       "      <th>Water_Level_(ftMSL)</th>\n",
       "      <th>Total_Storage_(MCM)</th>\n",
       "      <th>Active_Storage_(MCM)</th>\n",
       "      <th>Gross_Storage_Percentage_(%)</th>\n",
       "      <th>Net_Percentage_(%)</th>\n",
       "      <th>Energy_(MWh)</th>\n",
       "      <th>LB_Main_Canal_(MCM)</th>\n",
       "      <th>RB_Main_Canal_(MCM)</th>\n",
       "      <th>Main_Canals:_LB+RB_(MCM)</th>\n",
       "      <th>Spillway_(MCM)</th>\n",
       "      <th>Evaporation_(mm)</th>\n",
       "      <th>Evaporation_(MCM)</th>\n",
       "      <th>Inflow_(MCM)</th>\n",
       "      <th>Rainfall_(mm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1994-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1994-01-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1994-01-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1994-01-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1994-01-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date  Water_Level_(mMSL)  Water_Level_(ftMSL)  Total_Storage_(MCM)  \\\n",
       "0 1994-01-01                 NaN                  NaN                  NaN   \n",
       "1 1994-01-02                 NaN                  NaN                  NaN   \n",
       "2 1994-01-03                 NaN                  NaN                  NaN   \n",
       "3 1994-01-04                 NaN                  NaN                  NaN   \n",
       "4 1994-01-05                 NaN                  NaN                  NaN   \n",
       "\n",
       "   Active_Storage_(MCM)  Gross_Storage_Percentage_(%)  Net_Percentage_(%)  \\\n",
       "0                   NaN                           NaN                 NaN   \n",
       "1                   NaN                           NaN                 NaN   \n",
       "2                   NaN                           NaN                 NaN   \n",
       "3                   NaN                           NaN                 NaN   \n",
       "4                   NaN                           NaN                 NaN   \n",
       "\n",
       "   Energy_(MWh)  LB_Main_Canal_(MCM)  RB_Main_Canal_(MCM)  \\\n",
       "0           NaN                  NaN                  NaN   \n",
       "1           NaN                  NaN                  NaN   \n",
       "2           NaN                  NaN                  NaN   \n",
       "3           NaN                  NaN                  NaN   \n",
       "4           NaN                  NaN                  NaN   \n",
       "\n",
       "   Main_Canals:_LB+RB_(MCM)  Spillway_(MCM)  Evaporation_(mm)  \\\n",
       "0                       NaN             NaN               NaN   \n",
       "1                       NaN             NaN               NaN   \n",
       "2                       NaN             NaN               NaN   \n",
       "3                       NaN             NaN               NaN   \n",
       "4                       NaN             NaN               NaN   \n",
       "\n",
       "   Evaporation_(MCM)  Inflow_(MCM)  Rainfall_(mm)  \n",
       "0                NaN           NaN            NaN  \n",
       "1                NaN           NaN            NaN  \n",
       "2                NaN           NaN            NaN  \n",
       "3                NaN           NaN            NaN  \n",
       "4                NaN           NaN            NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load your hydrological data\n",
    "DATA_PATH = 'data/01. Hydrological Data 1994 to 2025.xlsx'\n",
    "\n",
    "df = pd.read_excel(DATA_PATH, header=2)\n",
    "\n",
    "# Clean column names\n",
    "df.columns = [col.replace('\\n', '_').replace(' ', '_') for col in df.columns]\n",
    "\n",
    "print(f'Dataset Shape: {df.shape}')\n",
    "print(f'\\nColumns: {list(df.columns)}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATA QUALITY REPORT\n",
      "============================================================\n",
      "\n",
      "Date Range: 1994-01-01 00:00:00 to 1994-12-31 00:00:00\n",
      "Total Records: 365\n",
      "\n",
      "Missing Values:\n",
      "                              Missing       %\n",
      "Date                                0    0.00\n",
      "Water_Level_(mMSL)                160   43.84\n",
      "Water_Level_(ftMSL)               365  100.00\n",
      "Total_Storage_(MCM)               365  100.00\n",
      "Active_Storage_(MCM)              365  100.00\n",
      "Gross_Storage_Percentage_(%)      365  100.00\n",
      "Net_Percentage_(%)                365  100.00\n",
      "Energy_(MWh)                      365  100.00\n",
      "LB_Main_Canal_(MCM)               365  100.00\n",
      "RB_Main_Canal_(MCM)               365  100.00\n",
      "Main_Canals:_LB+RB_(MCM)          365  100.00\n",
      "Spillway_(MCM)                    365  100.00\n",
      "Evaporation_(mm)                  365  100.00\n",
      "Evaporation_(MCM)                 365  100.00\n",
      "Inflow_(MCM)                      160   43.84\n",
      "Rainfall_(mm)                     160   43.84\n"
     ]
    }
   ],
   "source": [
    "# Data Quality Report\n",
    "print('='*60)\n",
    "print('DATA QUALITY REPORT')\n",
    "print('='*60)\n",
    "\n",
    "# Get date column (first column)\n",
    "date_col = df.columns[0]\n",
    "print(f'\\nDate Range: {df[date_col].min()} to {df[date_col].max()}')\n",
    "print(f'Total Records: {len(df)}')\n",
    "\n",
    "print('\\nMissing Values:')\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (missing / len(df) * 100).round(2)\n",
    "print(pd.DataFrame({'Missing': missing, '%': missing_pct}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric columns: ['Water_Level_(mMSL)', 'Water_Level_(ftMSL)', 'Total_Storage_(MCM)', 'Active_Storage_(MCM)', 'Gross_Storage_Percentage_(%)', 'Net_Percentage_(%)', 'Energy_(MWh)', 'LB_Main_Canal_(MCM)', 'RB_Main_Canal_(MCM)', 'Main_Canals:_LB+RB_(MCM)', 'Spillway_(MCM)', 'Evaporation_(mm)', 'Evaporation_(MCM)', 'Inflow_(MCM)', 'Rainfall_(mm)']\n"
     ]
    }
   ],
   "source": [
    "# Rename columns for easier access\n",
    "df = df.rename(columns={df.columns[0]: 'date'})\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df = df.set_index('date').sort_index()\n",
    "\n",
    "# Identify numeric columns\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(f'Numeric columns: {numeric_cols}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values before imputation: 4860\n",
      "\\nColumns with all NaN (will be dropped):\n",
      "['Water_Level_(ftMSL)', 'Total_Storage_(MCM)', 'Active_Storage_(MCM)', 'Gross_Storage_Percentage_(%)', 'Net_Percentage_(%)', 'Energy_(MWh)', 'LB_Main_Canal_(MCM)', 'RB_Main_Canal_(MCM)', 'Main_Canals:_LB+RB_(MCM)', 'Spillway_(MCM)', 'Evaporation_(mm)', 'Evaporation_(MCM)']\n",
      "\\nShape after dropping all-NaN columns: (365, 3)\n",
      "Numeric columns after cleanup: ['Water_Level_(mMSL)', 'Inflow_(MCM)', 'Rainfall_(mm)']...\n",
      "\\nMissing after imputation: 0\n"
     ]
    }
   ],
   "source": [
    "# Impute missing values - focus on columns that have some data\n",
    "print(f'Missing values before imputation: {df.isnull().sum().sum()}')\n",
    "print(f'\\\\nColumns with all NaN (will be dropped):')\n",
    "all_nan_cols = df.columns[df.isnull().all()].tolist()\n",
    "print(all_nan_cols)\n",
    "\n",
    "# Drop columns that are entirely NaN\n",
    "df = df.dropna(axis=1, how='all')\n",
    "print(f'\\\\nShape after dropping all-NaN columns: {df.shape}')\n",
    "\n",
    "# Update numeric_cols to exclude the columns we just dropped\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(f'Numeric columns after cleanup: {numeric_cols[:5]}...')  # Show first 5\n",
    "\n",
    "# Impute remaining missing values in numeric columns\n",
    "for col in numeric_cols:\n",
    "    missing_pct = df[col].isnull().sum() / len(df) * 100\n",
    "    if missing_pct > 0:\n",
    "        # For columns with some data, interpolate\n",
    "        df[col] = df[col].interpolate(method='linear', limit=7)\n",
    "        # Forward fill then backward fill\n",
    "        df[col] = df[col].bfill().ffill()\n",
    "        # Fill any remaining with mean\n",
    "        if df[col].isnull().sum() > 0:\n",
    "            df[col] = df[col].fillna(df[col].mean())\n",
    "\n",
    "print(f'\\\\nMissing after imputation: {df.isnull().sum().sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target variable: Water_Level_(mMSL)\n",
      "Features created. Shape: (365, 13)\n",
      "Missing values in df:\n",
      "Water_Level_(mMSL)                0\n",
      "Inflow_(MCM)                      0\n",
      "Rainfall_(mm)                     0\n",
      "day_of_year                       0\n",
      "month                             0\n",
      "week                              0\n",
      "month_sin                         0\n",
      "month_cos                         0\n",
      "Water_Level_(mMSL)_lag_1          1\n",
      "Water_Level_(mMSL)_lag_3          3\n",
      "Water_Level_(mMSL)_lag_7          7\n",
      "Water_Level_(mMSL)_roll_mean_7    0\n",
      "Water_Level_(mMSL)_roll_std_7     1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Set target column - adjust based on your data\n",
    "# Options: inflow, total_storage, water_level, etc.\n",
    "TARGET = numeric_cols[0]  # Use first numeric column as default\n",
    "print(f'Target variable: {TARGET}')\n",
    "\n",
    "# Create temporal features\n",
    "df['day_of_year'] = df.index.dayofyear\n",
    "df['month'] = df.index.month\n",
    "df['week'] = df.index.isocalendar().week.astype(int)\n",
    "\n",
    "# Cyclical encoding\n",
    "df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "\n",
    "# Lag features - adjust these based on your data availability\n",
    "# For shorter datasets, use fewer/shorter lags\n",
    "lag_days = [1, 3, 7]  # Reduced from [1, 3, 7, 14] to avoid excessive NaN rows\n",
    "for lag in lag_days:\n",
    "    df[f'{TARGET}_lag_{lag}'] = df[TARGET].shift(lag)\n",
    "\n",
    "# Rolling statistics - with min_periods to avoid excessive NaN\n",
    "df[f'{TARGET}_roll_mean_7'] = df[TARGET].rolling(window=7, min_periods=1).mean()\n",
    "df[f'{TARGET}_roll_std_7'] = df[TARGET].rolling(window=7, min_periods=1).std()\n",
    "\n",
    "print(f'Features created. Shape: {df.shape}')\n",
    "print(f'Missing values in df:')\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset: 365 rows\n",
      "After dropping NaN: 358 rows (dropped 7)\n",
      "\n",
      "Train-Test Split:\n",
      "  Training: 286 samples\n",
      "  Testing: 72 samples\n",
      "\n",
      "Feature matrices:\n",
      "  X_train shape: (286, 12)\n",
      "  X_test shape: (72, 12)\n",
      "  Features: 12\n"
     ]
    }
   ],
   "source": [
    "# Drop NaN rows from lag features and rolling statistics\n",
    "# Keep track of original size for diagnostics\n",
    "original_size = len(df)\n",
    "df_model = df.dropna()\n",
    "dropped_rows = original_size - len(df_model)\n",
    "print(f'Original dataset: {original_size} rows')\n",
    "print(f'After dropping NaN: {len(df_model)} rows (dropped {dropped_rows})')\n",
    "\n",
    "# Ensure we have enough data\n",
    "if len(df_model) == 0:\n",
    "    raise ValueError(\"All data was dropped. Check if dataset is large enough and lag features aren't too aggressive.\")\n",
    "\n",
    "# Time-based split (80/20)\n",
    "train_size = int(len(df_model) * 0.8)\n",
    "train_df = df_model.iloc[:train_size]\n",
    "test_df = df_model.iloc[train_size:]\n",
    "\n",
    "print(f'\\nTrain-Test Split:')\n",
    "print(f'  Training: {len(train_df)} samples')\n",
    "print(f'  Testing: {len(test_df)} samples')\n",
    "\n",
    "# Separate features and target\n",
    "feature_cols = [c for c in df_model.columns if c != TARGET]\n",
    "\n",
    "X_train, y_train = train_df[feature_cols], train_df[TARGET]\n",
    "X_test, y_test = test_df[feature_cols], test_df[TARGET]\n",
    "\n",
    "# Final validation\n",
    "if len(X_train) == 0:\n",
    "    raise ValueError(\"Training set is empty. Consider reducing lag features or using a larger dataset.\")\n",
    "\n",
    "print(f'\\nFeature matrices:')\n",
    "print(f'  X_train shape: {X_train.shape}')\n",
    "print(f'  X_test shape: {X_test.shape}')\n",
    "print(f'  Features: {len(feature_cols)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling data...\n",
      "  X_train: (286, 12), y_train: (286,)\n",
      "  X_test: (72, 12), y_test: (72,)\n",
      "âœ“ Data scaled successfully!\n",
      "  X_train_scaled shape: (286, 12)\n",
      "  X_test_scaled shape: (72, 12)\n"
     ]
    }
   ],
   "source": [
    "# Scale data\n",
    "print('Scaling data...')\n",
    "print(f'  X_train: {X_train.shape}, y_train: {y_train.shape}')\n",
    "print(f'  X_test: {X_test.shape}, y_test: {y_test.shape}')\n",
    "\n",
    "if len(X_train) == 0:\n",
    "    raise ValueError(\"Cannot scale empty training data. Check previous cells for data loading issues.\")\n",
    "\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "X_test_scaled = scaler_X.transform(X_test)\n",
    "\n",
    "y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1))\n",
    "y_test_scaled = scaler_y.transform(y_test.values.reshape(-1, 1))\n",
    "\n",
    "print('âœ“ Data scaled successfully!')\n",
    "print(f'  X_train_scaled shape: {X_train_scaled.shape}')\n",
    "print(f'  X_test_scaled shape: {X_test_scaled.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model 1: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Results:\n",
      "  RMSE: 3.1813\n",
      "  MAE: 3.0068\n",
      "  RÂ²: -3.0260\n"
     ]
    }
   ],
   "source": [
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=15,\n",
    "    min_samples_split=5,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "rf_pred = rf_model.predict(X_test_scaled)\n",
    "\n",
    "rf_rmse = np.sqrt(mean_squared_error(y_test, rf_pred))\n",
    "rf_mae = mean_absolute_error(y_test, rf_pred)\n",
    "rf_r2 = r2_score(y_test, rf_pred)\n",
    "\n",
    "print('Random Forest Results:')\n",
    "print(f'  RMSE: {rf_rmse:.4f}')\n",
    "print(f'  MAE: {rf_mae:.4f}')\n",
    "print(f'  RÂ²: {rf_r2:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Features:\n",
      "                           feature  importance\n",
      "7         Water_Level_(mMSL)_lag_1    0.492732\n",
      "2                      day_of_year    0.196514\n",
      "10  Water_Level_(mMSL)_roll_mean_7    0.122654\n",
      "9         Water_Level_(mMSL)_lag_7    0.075445\n",
      "8         Water_Level_(mMSL)_lag_3    0.065232\n",
      "4                             week    0.037798\n",
      "11   Water_Level_(mMSL)_roll_std_7    0.008612\n",
      "3                            month    0.000635\n",
      "0                     Inflow_(MCM)    0.000208\n",
      "1                    Rainfall_(mm)    0.000126\n"
     ]
    }
   ],
   "source": [
    "# Feature Importance\n",
    "feat_imp = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print('Top 10 Features:')\n",
    "print(feat_imp.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model 2: Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Results:\n",
      "  RMSE: 2.8763\n",
      "  MAE: 2.7027\n",
      "  RÂ²: -2.2912\n"
     ]
    }
   ],
   "source": [
    "gb_model = GradientBoostingRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "gb_model.fit(X_train_scaled, y_train)\n",
    "gb_pred = gb_model.predict(X_test_scaled)\n",
    "\n",
    "gb_rmse = np.sqrt(mean_squared_error(y_test, gb_pred))\n",
    "gb_mae = mean_absolute_error(y_test, gb_pred)\n",
    "gb_r2 = r2_score(y_test, gb_pred)\n",
    "\n",
    "print('Gradient Boosting Results:')\n",
    "print(f'  RMSE: {gb_rmse:.4f}')\n",
    "print(f'  MAE: {gb_mae:.4f}')\n",
    "print(f'  RÂ²: {gb_r2:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model 3: LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM shapes: X_train=(272, 14, 12), X_test=(58, 14, 12)\n"
     ]
    }
   ],
   "source": [
    "# Create sequences for LSTM\n",
    "def create_sequences(X, y, seq_length=14):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - seq_length):\n",
    "        Xs.append(X[i:(i + seq_length)])\n",
    "        ys.append(y[i + seq_length])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "SEQ_LEN = 14\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled.flatten(), SEQ_LEN)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_scaled.flatten(), SEQ_LEN)\n",
    "\n",
    "print(f'LSTM shapes: X_train={X_train_seq.shape}, X_test={X_test_seq.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">19,712</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,416</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              â”‚            <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)         â”‚        \u001b[38;5;34m19,712\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (\u001b[38;5;33mDropout\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)         â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             â”‚        \u001b[38;5;34m12,416\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             â”‚           \u001b[38;5;34m528\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              â”‚            \u001b[38;5;34m17\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">32,673</span> (127.63 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m32,673\u001b[0m (127.63 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">32,673</span> (127.63 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m32,673\u001b[0m (127.63 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build LSTM\n",
    "lstm_model = Sequential([\n",
    "    LSTM(64, activation='relu', return_sequences=True, input_shape=(SEQ_LEN, X_train_seq.shape[2])),\n",
    "    Dropout(0.2),\n",
    "    LSTM(32, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "lstm_model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 172ms/step - loss: 0.5954 - mae: 0.7242 - val_loss: 0.0319 - val_mae: 0.1683\n",
      "Epoch 2/100\n",
      "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.1562 - mae: 0.3239 - val_loss: 0.1953 - val_mae: 0.4340\n",
      "Epoch 3/100\n",
      "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.0808 - mae: 0.2255 - val_loss: 0.0768 - val_mae: 0.2522\n",
      "Epoch 4/100\n",
      "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - loss: 0.0735 - mae: 0.2238 - val_loss: 0.0545 - val_mae: 0.2132\n",
      "Epoch 5/100\n",
      "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.0530 - mae: 0.1846 - val_loss: 0.0529 - val_mae: 0.2070\n",
      "Epoch 6/100\n",
      "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 0.0437 - mae: 0.1639 - val_loss: 0.0206 - val_mae: 0.1300\n",
      "Epoch 7/100\n",
      "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.0325 - mae: 0.1310 - val_loss: 0.0204 - val_mae: 0.1163\n",
      "Epoch 8/100\n",
      "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0289 - mae: 0.1366 - val_loss: 0.0257 - val_mae: 0.1171\n",
      "Epoch 9/100\n",
      "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0272 - mae: 0.1240 - val_loss: 0.0384 - val_mae: 0.1365\n",
      "Epoch 10/100\n",
      "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0248 - mae: 0.1156 - val_loss: 0.0326 - val_mae: 0.1305\n",
      "Epoch 11/100\n",
      "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0225 - mae: 0.1205 - val_loss: 0.0500 - val_mae: 0.1478\n",
      "Epoch 12/100\n",
      "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0179 - mae: 0.1003 - val_loss: 0.0446 - val_mae: 0.1425\n",
      "Epoch 13/100\n",
      "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0230 - mae: 0.1148 - val_loss: 0.0440 - val_mae: 0.1406\n",
      "Epoch 14/100\n",
      "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0203 - mae: 0.1019 - val_loss: 0.0425 - val_mae: 0.1389\n",
      "Epoch 15/100\n",
      "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0199 - mae: 0.1069 - val_loss: 0.0430 - val_mae: 0.1377\n",
      "Epoch 16/100\n",
      "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0160 - mae: 0.0955 - val_loss: 0.0400 - val_mae: 0.1359\n",
      "Epoch 17/100\n",
      "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0169 - mae: 0.0977 - val_loss: 0.0395 - val_mae: 0.1349\n"
     ]
    }
   ],
   "source": [
    "# Train LSTM\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "history = lstm_model.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 367ms/step\n",
      "LSTM Results:\n",
      "  RMSE: 2.1945\n",
      "  MAE: 2.1028\n",
      "  RÂ²: -5.6267\n"
     ]
    }
   ],
   "source": [
    "# LSTM Evaluation\n",
    "lstm_pred_scaled = lstm_model.predict(X_test_seq)\n",
    "lstm_pred = scaler_y.inverse_transform(lstm_pred_scaled)\n",
    "y_test_actual = scaler_y.inverse_transform(y_test_seq.reshape(-1, 1))\n",
    "\n",
    "lstm_rmse = np.sqrt(mean_squared_error(y_test_actual, lstm_pred))\n",
    "lstm_mae = mean_absolute_error(y_test_actual, lstm_pred)\n",
    "lstm_r2 = r2_score(y_test_actual, lstm_pred)\n",
    "\n",
    "print('LSTM Results:')\n",
    "print(f'  RMSE: {lstm_rmse:.4f}')\n",
    "print(f'  MAE: {lstm_mae:.4f}')\n",
    "print(f'  RÂ²: {lstm_r2:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Probabilistic Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantile 0.1 trained\n",
      "Quantile 0.5 trained\n",
      "Quantile 0.9 trained\n",
      "\n",
      "80% Interval Coverage: 8.33%\n"
     ]
    }
   ],
   "source": [
    "# Quantile Regression for uncertainty estimation\n",
    "quantiles = [0.1, 0.5, 0.9]\n",
    "q_models = {}\n",
    "q_preds = {}\n",
    "\n",
    "for q in quantiles:\n",
    "    model = GradientBoostingRegressor(\n",
    "        loss='quantile', alpha=q,\n",
    "        n_estimators=100, max_depth=5, random_state=42\n",
    "    )\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    q_models[q] = model\n",
    "    q_preds[q] = model.predict(X_test_scaled)\n",
    "    print(f'Quantile {q} trained')\n",
    "\n",
    "# Coverage check\n",
    "coverage = ((y_test.values >= q_preds[0.1]) & (y_test.values <= q_preds[0.9])).mean()\n",
    "print(f'\\n80% Interval Coverage: {coverage:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "MODEL COMPARISON\n",
      "==================================================\n",
      "            Model     RMSE      MAE        RÂ²\n",
      "             LSTM 2.194475 2.102814 -5.626746\n",
      "Gradient Boosting 2.876325 2.702729 -2.291157\n",
      "    Random Forest 3.181277 3.006846 -3.026018\n",
      "\n",
      "ğŸ† Best: LSTM\n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame({\n",
    "    'Model': ['Random Forest', 'Gradient Boosting', 'LSTM'],\n",
    "    'RMSE': [rf_rmse, gb_rmse, lstm_rmse],\n",
    "    'MAE': [rf_mae, gb_mae, lstm_mae],\n",
    "    'RÂ²': [rf_r2, gb_r2, lstm_r2]\n",
    "}).sort_values('RMSE')\n",
    "\n",
    "print('='*50)\n",
    "print('MODEL COMPARISON')\n",
    "print('='*50)\n",
    "print(results.to_string(index=False))\n",
    "print(f'\\nğŸ† Best: {results.iloc[0][\"Model\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All models saved to ./models/\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Save models\n",
    "joblib.dump(rf_model, 'models/random_forest.pkl')\n",
    "joblib.dump(gb_model, 'models/gradient_boosting.pkl')\n",
    "joblib.dump(scaler_X, 'models/scaler_X.pkl')\n",
    "joblib.dump(scaler_y, 'models/scaler_y.pkl')\n",
    "for q, m in q_models.items():\n",
    "    joblib.dump(m, f'models/quantile_{int(q*100)}.pkl')\n",
    "lstm_model.save('models/lstm_model.keras')\n",
    "\n",
    "# Save results\n",
    "results.to_csv('model_comparison.csv', index=False)\n",
    "\n",
    "print('All models saved to ./models/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
