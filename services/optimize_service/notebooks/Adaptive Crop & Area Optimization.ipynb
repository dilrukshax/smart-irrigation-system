{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96a1f847",
   "metadata": {},
   "source": [
    "# Adaptive Crop & Area Optimization (ACA-O) System\n",
    "## Data Distribution, Variance & Model Generalization Analysis\n",
    "\n",
    "**4th Year Project - Sri Lanka Agricultural Optimization**\n",
    "\n",
    "This notebook provides comprehensive analysis of multi-source agricultural data for building an intelligent crop optimization system.\n",
    "\n",
    "---\n",
    "\n",
    "### Objectives\n",
    "1. **Data Distribution Analysis** - Statistical and visual exploration of all 6 datasets\n",
    "2. **Crop Recommendation** - Region and climate-based optimal crop selection\n",
    "3. **Yield Prediction** - Environmental factor-driven yield forecasting\n",
    "4. **Area Optimization** - Linear programming for land allocation\n",
    "5. **Price Forecasting** - LSTM-based market price prediction\n",
    "6. **Model Generalization** - Cross-validation and temporal/spatial splits\n",
    "\n",
    "---\n",
    "\n",
    "### Datasets Overview\n",
    "\n",
    "| # | Dataset | Type | Records | ACA-O Module | Key Features |\n",
    "|---|---------|------|---------|--------------|--------------|\n",
    "| 1 | **Paddy Cultivation Statistics** | Yield/Area | 28 districts | Yield Prediction | District-wise sown/harvested areas, yields by irrigation scheme |\n",
    "| 2 | **Economy & Rice Production (1960-2020)** | Economic | 61 years | Economic Context | GDP, inflation, population, rice production trends |\n",
    "| 3 | **Vegetable Price Forecasting** | Market Prices | Variable | Price Forecasting | Multi-market vegetable price data |\n",
    "| 4 | **Historical Veg & Fruit Prices** | Prices + Climate | 130,000+ | Main Training Data | Region, climate, yield impact, prices across 25 districts |\n",
    "| 5 | **SriOryzia Rice Time Series** | Time Series | 324 months | Price LSTM | Monthly rice prices, production, exchange rates (1996-2022) |\n",
    "| 6 | **Climate & Meteorology** | Weather | Variable | Climate Risk | Temperature, rainfall, humidity patterns |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044790a0",
   "metadata": {},
   "source": [
    "## 1. Configuration & Imports\n",
    "\n",
    "Consolidated imports and global settings for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c12b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# IMPORTS & GLOBAL CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "import glob\n",
    "import math\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "# Data & Numerical\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "# Machine Learning - sklearn\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Deep Learning - PyTorch\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =============================================================================\n",
    "# GLOBAL SETTINGS\n",
    "# =============================================================================\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('default')\n",
    "sns.set_theme(style='whitegrid', palette='husl')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['axes.titlesize'] = 12\n",
    "plt.rcParams['axes.labelsize'] = 10\n",
    "\n",
    "# Device configuration\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"Random Seed: {RANDOM_SEED}\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"NumPy: {np.__version__}\")\n",
    "print(f\"Pandas: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2760f915",
   "metadata": {},
   "source": [
    "## 2. Data Acquisition & PDF Extraction\n",
    "\n",
    "This section downloads all Sri Lanka agricultural datasets from:\n",
    "1. **Kaggle** - CSV datasets (paddy, climate, prices, economy)\n",
    "2. **Official Government PDFs** - DCS, HARTI, CBSL, Irrigation Dept\n",
    "3. **Direct PDF URLs** - Hydrological reports, agro-ecological maps\n",
    "\n",
    "**Requirements:**\n",
    "- Kaggle API: Place `kaggle.json` in `~/.kaggle/` directory\n",
    "- Java JRE/JDK: Required for tabula-py PDF extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace03b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 2.1 Install Dependencies & Setup Directories\n",
    "# =============================================================================\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Install required packages (run once)\n",
    "subprocess.check_call([\n",
    "    sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n",
    "    \"kaggle\", \"requests\", \"beautifulsoup4\", \"tabula-py\", \"pdfplumber\"\n",
    "])\n",
    "\n",
    "import requests\n",
    "from urllib.parse import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "import zipfile\n",
    "\n",
    "# Directory layout\n",
    "BASE_DIR      = Path(\"data\")\n",
    "KAGGLE_DIR    = BASE_DIR / \"kaggle\"\n",
    "PDF_DIR       = BASE_DIR / \"pdf\"\n",
    "EXTRACTED_DIR = BASE_DIR / \"pdf_extracted\"\n",
    "PROCESSED_DIR = BASE_DIR / \"processed\"\n",
    "RAW_DIR       = BASE_DIR / \"raw\"\n",
    "MODELS_DIR    = Path(\"../app/models\")\n",
    "\n",
    "for d in [BASE_DIR, KAGGLE_DIR, PDF_DIR, EXTRACTED_DIR, PROCESSED_DIR, RAW_DIR, MODELS_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Data Directory Structure:\")\n",
    "print(f\"  Base:      {BASE_DIR.resolve()}\")\n",
    "print(f\"  Kaggle:    {KAGGLE_DIR.resolve()}\")\n",
    "print(f\"  PDFs:      {PDF_DIR.resolve()}\")\n",
    "print(f\"  Extracted: {EXTRACTED_DIR.resolve()}\")\n",
    "print(f\"  Processed: {PROCESSED_DIR.resolve()}\")\n",
    "print(f\"  Models:    {MODELS_DIR.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079600a8",
   "metadata": {},
   "source": [
    "### 2.2 Dataset Configuration\n",
    "\n",
    "**Kaggle Datasets (Sri Lanka Only):**\n",
    "| Dataset | Description |\n",
    "|---------|-------------|\n",
    "| paddy_cultivation_sl | Paddy cultivation statistics by district |\n",
    "| rice_time_series_sl | Multivariate rice price forecasting data |\n",
    "| veg_fruit_prices_sl | Historical vegetable and fruit prices |\n",
    "| veg_price_forecasting_sl | Vegetable price forecasting data |\n",
    "| weather_sl | Sri Lanka weather dataset |\n",
    "| economy_sl | Economy of Sri Lanka indicators |\n",
    "| climate_data_sl | Sri Lanka climate data |\n",
    "| economy_rice_sl | Economy and rice production 1960-2020 |\n",
    "\n",
    "**Official PDF Sources:**\n",
    "- DCS Paddy Statistics (all seasons)\n",
    "- HARTI Daily & Monthly Food Commodities Bulletins\n",
    "- CBSL Agriculture Data Bulletin\n",
    "- Irrigation Dept Reservoirs Status\n",
    "- DOA Soil Fertility Maps\n",
    "- Hydrological Annual Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce4f64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 2.2.1 Kaggle Datasets Configuration\n",
    "# =============================================================================\n",
    "\n",
    "KAGGLE_DATASETS = {\n",
    "    # Paddy & Rice\n",
    "    \"paddy_cultivation_sl\":     \"tharindumadhusanka9/paddy-cultivation-statics-in-sri-lanka\",\n",
    "    \"rice_time_series_sl\":      \"luqmanrumaiz/srioryzia-multivariate-rice-price-forecasting\",\n",
    "    \"economy_rice_sl\":          \"namalrathnayake1990/economy-and-rice-production-sri-lanka-1960-2020\",\n",
    "    \n",
    "    # Vegetables & Fruits\n",
    "    \"veg_fruit_prices_sl\":      \"isuranga/historical-vegetable-and-fruit-prices-in-sri-lanka\",\n",
    "    \"veg_price_forecasting_sl\": \"nisith210144g/vegi-price\",\n",
    "    \n",
    "    # Climate & Weather\n",
    "    \"weather_sl\":               \"rasulmah/sri-lanka-weather-dataset\",\n",
    "    \"climate_data_sl\":          \"tharindumadhusanka9/sri-lanka-climate-data\",\n",
    "    \n",
    "    # Economy\n",
    "    \"economy_sl\":               \"amritharj/economy-of-sri-lanka\",\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# 2.2.2 Official PDF Pages (Government Sources)\n",
    "# =============================================================================\n",
    "\n",
    "OFFICIAL_PDF_PAGES = {\n",
    "    # Department of Census & Statistics â€“ Paddy Statistics\n",
    "    \"dcs_paddy_statistics\":\n",
    "        \"https://www.statistics.gov.lk/Agriculture/StaticalInformation/Paddy_Statistics\",\n",
    "\n",
    "    # HARTI price bulletins â€“ daily & monthly food commodities\n",
    "    \"harti_daily\":\n",
    "        \"https://www.harti.gov.lk/index.php/en/market-information/data-food-commodities-bulletin\",\n",
    "    \"harti_monthly\":\n",
    "        \"https://www.harti.gov.lk/index.php/en/market-information/monthly-food-commodities-bulletin\",\n",
    "\n",
    "    # Central Bank â€“ Agriculture Data Bulletin\n",
    "    \"cbsl_agri_bulletin\":\n",
    "        \"https://www.cbsl.gov.lk/en/agriculture-data-bulletin\",\n",
    "\n",
    "    # Irrigation Dept â€“ reservoirs status\n",
    "    \"irrigation_reservoirs\":\n",
    "        \"https://irrigation.gov.lk/web/index.php?Itemid=101&catid=33&id=84%3Areservoirs&lang=en&option=com_content&view=article\",\n",
    "\n",
    "    # DOA â€“ soil fertility maps\n",
    "    \"doa_soil_fertility\":\n",
    "        \"https://doa.gov.lk/soil-fertility-map/\",\n",
    "\n",
    "    # NSDI â€“ thematic maps\n",
    "    \"nsdi_thematic_maps\":\n",
    "        \"https://nsdi.gov.lk/thematic-maps\",\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# 2.2.3 Direct PDF URLs (Known Important Documents)\n",
    "# =============================================================================\n",
    "\n",
    "DIRECT_PDF_URLS = {\n",
    "    \"irrigation_hydrological_annual_2018_19\":\n",
    "        \"https://www.irrigation.gov.lk/web/images/Hydrological-Annual/12_Hydrological_Annual_18-19.pdf\",\n",
    "    \"irrigation_hydrological_annual_2020_21\":\n",
    "        \"https://www.irrigation.gov.lk/web/images/Publications/Hydrologyical_Annual_2020-21.pdf\",\n",
    "    \"agro_ecological_regions_jrc\":\n",
    "        \"https://esdac.jrc.ec.europa.eu/images/Eudasm/Asia/images/maps/download/PDF/LK2007_CL.pdf\",\n",
    "}\n",
    "\n",
    "print(f\"Configured {len(KAGGLE_DATASETS)} Kaggle datasets\")\n",
    "print(f\"Configured {len(OFFICIAL_PDF_PAGES)} official PDF pages\")\n",
    "print(f\"Configured {len(DIRECT_PDF_URLS)} direct PDF URLs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e650e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 2.3 Download Functions\n",
    "# =============================================================================\n",
    "\n",
    "def download_kaggle_datasets(kaggle_map: dict, out_dir: Path):\n",
    "    \"\"\"\n",
    "    Download and unzip all Kaggle datasets into out_dir/<nickname>\n",
    "    Requires: ~/.kaggle/kaggle.json\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"DOWNLOADING KAGGLE DATASETS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for nickname, slug in kaggle_map.items():\n",
    "        target = out_dir / nickname\n",
    "        target.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Skip if already downloaded\n",
    "        if list(target.glob('*.csv')):\n",
    "            print(f\"âœ“ {nickname}: Already exists\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nâ†’ {nickname}: {slug}\")\n",
    "        try:\n",
    "            subprocess.run(\n",
    "                [\"kaggle\", \"datasets\", \"download\", \"-d\", slug, \"-p\", str(target), \"--unzip\"],\n",
    "                check=True,\n",
    "                capture_output=True\n",
    "            )\n",
    "            print(f\"  âœ“ Downloaded successfully\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"  âœ— Failed: {e}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"  âœ— Kaggle CLI not found. Install with: pip install kaggle\")\n",
    "            break\n",
    "    \n",
    "    print(\"\\n[Kaggle] Finished downloading Sri Lanka datasets.\")\n",
    "\n",
    "\n",
    "def download_pdfs_from_page(name: str, url: str, base_out_dir: Path, timeout: int = 40):\n",
    "    \"\"\"\n",
    "    Fetch a web page and download ALL linked .pdf files.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"SCRAPING: {name}\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "    print(f\"URL: {url}\")\n",
    "\n",
    "    out_dir = base_out_dir / name\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}\n",
    "        resp = requests.get(url, timeout=timeout, headers=headers)\n",
    "        resp.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Failed to fetch page: {e}\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    links = soup.find_all(\"a\", href=True)\n",
    "    pdf_links = []\n",
    "\n",
    "    for a in links:\n",
    "        href = a[\"href\"]\n",
    "        if \".pdf\" in href.lower():\n",
    "            full_url = urljoin(url, href)\n",
    "            pdf_links.append(full_url)\n",
    "\n",
    "    pdf_links = sorted(set(pdf_links))\n",
    "    \n",
    "    if not pdf_links:\n",
    "        print(\"  (No PDF links found on this page)\")\n",
    "        return\n",
    "\n",
    "    print(f\"  Found {len(pdf_links)} PDF file(s)\")\n",
    "    \n",
    "    for pdf_url in pdf_links:\n",
    "        filename = pdf_url.split(\"/\")[-1].split(\"?\")[0]\n",
    "        # Clean filename\n",
    "        filename = filename.replace(\"%20\", \"_\").replace(\" \", \"_\")\n",
    "        dest = out_dir / filename\n",
    "        \n",
    "        if dest.exists():\n",
    "            print(f\"  âœ“ {filename}: Already exists\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            print(f\"  â†’ Downloading: {filename}\")\n",
    "            with requests.get(pdf_url, stream=True, timeout=timeout, headers=headers) as r:\n",
    "                r.raise_for_status()\n",
    "                with open(dest, \"wb\") as f:\n",
    "                    for chunk in r.iter_content(chunk_size=8192):\n",
    "                        if chunk:\n",
    "                            f.write(chunk)\n",
    "            print(f\"    âœ“ Saved\")\n",
    "        except Exception as e:\n",
    "            print(f\"    âœ— Failed: {e}\")\n",
    "\n",
    "\n",
    "def download_direct_pdfs(pdf_map: dict, base_out_dir: Path, timeout: int = 40):\n",
    "    \"\"\"\n",
    "    Download direct PDF URLs into base_out_dir/direct_pdfs\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"DOWNLOADING DIRECT PDF URLS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    misc_dir = base_out_dir / \"direct_pdfs\"\n",
    "    misc_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}\n",
    "    \n",
    "    for name, url in pdf_map.items():\n",
    "        filename = f\"{name}.pdf\"\n",
    "        dest = misc_dir / filename\n",
    "        \n",
    "        if dest.exists():\n",
    "            print(f\"âœ“ {name}: Already exists\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            print(f\"â†’ Downloading: {name}\")\n",
    "            with requests.get(url, stream=True, timeout=timeout, headers=headers) as r:\n",
    "                r.raise_for_status()\n",
    "                with open(dest, \"wb\") as f:\n",
    "                    for chunk in r.iter_content(chunk_size=8192):\n",
    "                        if chunk:\n",
    "                            f.write(chunk)\n",
    "            print(f\"  âœ“ Saved\")\n",
    "        except Exception as e:\n",
    "            print(f\"  âœ— Failed: {e}\")\n",
    "\n",
    "\n",
    "def extract_tables_from_pdfs(pdf_root: Path, out_root: Path):\n",
    "    \"\"\"\n",
    "    Extract ALL tables from PDFs into CSV files using tabula-py.\n",
    "    Requires Java JRE/JDK installed.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"EXTRACTING TABLES FROM PDFs â†’ CSV\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        import tabula\n",
    "    except ImportError:\n",
    "        print(\"âœ— tabula-py not installed. Run: pip install tabula-py\")\n",
    "        return\n",
    "    \n",
    "    pdf_root = Path(pdf_root)\n",
    "    out_root = Path(out_root)\n",
    "    out_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    pdf_files = list(pdf_root.rglob(\"*.pdf\"))\n",
    "    \n",
    "    if not pdf_files:\n",
    "        print(\"  (No PDFs found to extract)\")\n",
    "        return\n",
    "\n",
    "    print(f\"  Found {len(pdf_files)} PDF file(s)\")\n",
    "    \n",
    "    for pdf_path in pdf_files:\n",
    "        rel_dir = pdf_path.parent.relative_to(pdf_root)\n",
    "        target_dir = out_root / rel_dir\n",
    "        target_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        print(f\"\\n  â†’ Processing: {pdf_path.name}\")\n",
    "        \n",
    "        try:\n",
    "            tables = tabula.read_pdf(\n",
    "                str(pdf_path),\n",
    "                pages=\"all\",\n",
    "                multiple_tables=True,\n",
    "                silent=True\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"    âœ— Failed to read: {e}\")\n",
    "            continue\n",
    "\n",
    "        if not tables:\n",
    "            print(\"    (No tables detected)\")\n",
    "            continue\n",
    "\n",
    "        print(f\"    Found {len(tables)} table(s)\")\n",
    "        \n",
    "        for i, df in enumerate(tables, start=1):\n",
    "            if df.empty:\n",
    "                continue\n",
    "            clean_stem = pdf_path.stem.replace(\" \", \"_\").replace(\"-\", \"_\")\n",
    "            out_csv = target_dir / f\"{clean_stem}_table{i}.csv\"\n",
    "            try:\n",
    "                df.to_csv(out_csv, index=False)\n",
    "                print(f\"    âœ“ Saved: {out_csv.name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"    âœ— Failed to save table {i}: {e}\")\n",
    "\n",
    "print(\"Download functions defined successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25abf92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 2.4 Execute Data Download Pipeline\n",
    "# =============================================================================\n",
    "\n",
    "def run_full_data_pipeline():\n",
    "    \"\"\"\n",
    "    Main function to download all datasets and extract PDFs.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"  ACA-O DATA ACQUISITION PIPELINE\")\n",
    "    print(\"  Sri Lanka Agricultural Optimization Datasets\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # 1) Download Kaggle CSV datasets\n",
    "    download_kaggle_datasets(KAGGLE_DATASETS, KAGGLE_DIR)\n",
    "    \n",
    "    # 2) Scrape and download PDFs from official government pages\n",
    "    for name, url in OFFICIAL_PDF_PAGES.items():\n",
    "        download_pdfs_from_page(name, url, PDF_DIR)\n",
    "    \n",
    "    # 3) Download direct PDF URLs (hydrology reports, agro-ecological maps)\n",
    "    download_direct_pdfs(DIRECT_PDF_URLS, PDF_DIR)\n",
    "    \n",
    "    # 4) Extract tables from ALL downloaded PDFs â†’ CSV\n",
    "    extract_tables_from_pdfs(PDF_DIR, EXTRACTED_DIR)\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"  PIPELINE COMPLETE\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Count files\n",
    "    kaggle_csvs = list(KAGGLE_DIR.rglob(\"*.csv\"))\n",
    "    pdf_files = list(PDF_DIR.rglob(\"*.pdf\"))\n",
    "    extracted_csvs = list(EXTRACTED_DIR.rglob(\"*.csv\"))\n",
    "    \n",
    "    print(f\"\\nðŸ“ Kaggle CSVs:     {len(kaggle_csvs)} files in {KAGGLE_DIR}\")\n",
    "    print(f\"ðŸ“„ Downloaded PDFs: {len(pdf_files)} files in {PDF_DIR}\")\n",
    "    print(f\"ðŸ“Š Extracted CSVs:  {len(extracted_csvs)} files in {EXTRACTED_DIR}\")\n",
    "    \n",
    "    return {\n",
    "        'kaggle_csvs': kaggle_csvs,\n",
    "        'pdf_files': pdf_files,\n",
    "        'extracted_csvs': extracted_csvs\n",
    "    }\n",
    "\n",
    "# Run the full pipeline\n",
    "pipeline_results = run_full_data_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3f6db2",
   "metadata": {},
   "source": [
    "## 2.3 Data Loading\n",
    "\n",
    "Load all downloaded datasets from Kaggle and extracted PDF tables into DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aebe789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3.1 CSV Loading Utilities\n",
    "# =============================================================================\n",
    "\n",
    "def load_csv_from_folder(folder: Path, prefer_largest: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load CSV file(s) from a folder.\n",
    "    If multiple CSVs exist, loads the largest one by default.\n",
    "    \"\"\"\n",
    "    csv_files = list(folder.glob('*.csv')) or list(folder.rglob('*.csv'))\n",
    "    \n",
    "    if not csv_files:\n",
    "        raise FileNotFoundError(f'No CSV files found in {folder}')\n",
    "    \n",
    "    if prefer_largest:\n",
    "        main_csv = max(csv_files, key=lambda p: p.stat().st_size)\n",
    "    else:\n",
    "        main_csv = csv_files[0]\n",
    "    \n",
    "    # Try different encodings\n",
    "    for encoding in ['utf-8', 'latin-1', 'cp1252', 'iso-8859-1']:\n",
    "        try:\n",
    "            df = pd.read_csv(main_csv, encoding=encoding, low_memory=False)\n",
    "            print(f\"  âœ“ Loaded {main_csv.name}: {df.shape[0]} rows Ã— {df.shape[1]} cols\")\n",
    "            return df\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    \n",
    "    # Fallback with error replacement\n",
    "    df = pd.read_csv(main_csv, encoding='utf-8', errors='replace', low_memory=False)\n",
    "    print(f\"  âœ“ Loaded {main_csv.name} (with encoding fallback): {df.shape}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_all_csvs_from_folder(folder: Path) -> dict:\n",
    "    \"\"\"\n",
    "    Load ALL CSV files from a folder into a dictionary.\n",
    "    \"\"\"\n",
    "    csv_files = list(folder.rglob('*.csv'))\n",
    "    results = {}\n",
    "    \n",
    "    for csv_file in csv_files:\n",
    "        try:\n",
    "            df = pd.read_csv(csv_file, encoding='utf-8', low_memory=False, errors='replace')\n",
    "            key = csv_file.stem\n",
    "            results[key] = df\n",
    "        except Exception as e:\n",
    "            print(f\"  âœ— Failed to load {csv_file.name}: {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"CSV loading utilities defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b72d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3.2 Load All Kaggle Datasets\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Loading Kaggle Datasets...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Primary datasets\n",
    "df_paddy = load_csv_from_folder(KAGGLE_DIR / 'paddy_cultivation_sl')\n",
    "df_rice_ts = load_csv_from_folder(KAGGLE_DIR / 'rice_time_series_sl')\n",
    "df_economy_rice = load_csv_from_folder(KAGGLE_DIR / 'economy_rice_sl')\n",
    "df_hist_veg = load_csv_from_folder(KAGGLE_DIR / 'veg_fruit_prices_sl')\n",
    "df_veg_price = load_csv_from_folder(KAGGLE_DIR / 'veg_price_forecasting_sl')\n",
    "df_weather = load_csv_from_folder(KAGGLE_DIR / 'weather_sl')\n",
    "df_climate = load_csv_from_folder(KAGGLE_DIR / 'climate_data_sl')\n",
    "df_economy = load_csv_from_folder(KAGGLE_DIR / 'economy_sl')\n",
    "\n",
    "# Store all in a dictionary for easy access\n",
    "DATASETS = {\n",
    "    'paddy': df_paddy,\n",
    "    'rice_ts': df_rice_ts,\n",
    "    'economy_rice': df_economy_rice,\n",
    "    'hist_veg': df_hist_veg,\n",
    "    'veg_price': df_veg_price,\n",
    "    'weather': df_weather,\n",
    "    'climate': df_climate,\n",
    "    'economy': df_economy,\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"KAGGLE DATASETS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "for name, df in DATASETS.items():\n",
    "    print(f\"  {name:15} : {df.shape[0]:>7,} rows Ã— {df.shape[1]:>3} cols\")\n",
    "print(f\"\\n  Total: {sum(df.shape[0] for df in DATASETS.values()):,} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65443485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3.3 Load Extracted PDF Tables\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Loading Extracted PDF Tables...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load all extracted CSVs from PDFs\n",
    "pdf_tables = load_all_csvs_from_folder(EXTRACTED_DIR)\n",
    "\n",
    "print(f\"\\nFound {len(pdf_tables)} extracted tables from PDFs\")\n",
    "\n",
    "# Categorize by source\n",
    "dcs_tables = {k: v for k, v in pdf_tables.items() if 'dcs' in k.lower() or 'paddy' in k.lower()}\n",
    "harti_tables = {k: v for k, v in pdf_tables.items() if 'harti' in k.lower()}\n",
    "irrigation_tables = {k: v for k, v in pdf_tables.items() if 'irrigation' in k.lower() or 'hydro' in k.lower()}\n",
    "other_tables = {k: v for k, v in pdf_tables.items() \n",
    "                if k not in dcs_tables and k not in harti_tables and k not in irrigation_tables}\n",
    "\n",
    "print(f\"\\nBy Category:\")\n",
    "print(f\"  DCS (Paddy/Census):  {len(dcs_tables)} tables\")\n",
    "print(f\"  HARTI (Prices):      {len(harti_tables)} tables\")\n",
    "print(f\"  Irrigation:          {len(irrigation_tables)} tables\")\n",
    "print(f\"  Other:               {len(other_tables)} tables\")\n",
    "\n",
    "# Preview extracted tables\n",
    "if pdf_tables:\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(\"Sample Extracted Tables:\")\n",
    "    for name, df in list(pdf_tables.items())[:5]:\n",
    "        print(f\"  â€¢ {name}: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69478d6b",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis\n",
    "\n",
    "Quick exploration of loaded datasets to understand their structure and contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64c0701",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eda(df: pd.DataFrame, name: str):\n",
    "    print(f'\\n{name}: {df.shape[0]} rows, {df.shape[1]} cols')\n",
    "    print(f'Columns: {list(df.columns)}')\n",
    "    display(df.head(3))\n",
    "\n",
    "eda(df_paddy, 'Paddy Stats')\n",
    "eda(df_economy, 'Economy & Rice')\n",
    "eda(df_hist_veg, 'Historical Prices')\n",
    "eda(df_rice_ts, 'Rice Time Series')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e172264",
   "metadata": {},
   "source": [
    "## 4. Data Distribution & Coverage Analysis\n",
    "\n",
    "This section provides statistical analysis of data distributions across all datasets, identifying:\n",
    "- Feature distributions and skewness\n",
    "- Temporal coverage and gaps\n",
    "- Regional representation\n",
    "- Class imbalances in categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d959b21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3.1 Dataset Statistics Summary\n",
    "# =============================================================================\n",
    "\n",
    "def dataset_summary(datasets_dict):\n",
    "    \"\"\"Generate statistical summary for all datasets.\"\"\"\n",
    "    summary_data = []\n",
    "    \n",
    "    for name, df in datasets_dict.items():\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        summary_data.append({\n",
    "            'Dataset': name,\n",
    "            'Records': len(df),\n",
    "            'Features': len(df.columns),\n",
    "            'Numeric': len(numeric_cols),\n",
    "            'Categorical': len(df.columns) - len(numeric_cols),\n",
    "            'Missing (%)': round(df.isnull().sum().sum() / (df.shape[0] * df.shape[1]) * 100, 2),\n",
    "            'Memory (KB)': round(df.memory_usage(deep=True).sum() / 1024, 1)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(summary_data)\n",
    "\n",
    "# Create datasets dictionary\n",
    "datasets = {\n",
    "    'Paddy Statistics': df_paddy,\n",
    "    'Economy (Rice)': df_economy,\n",
    "    'Vegetable Prices': df_veg_price,\n",
    "    'Historical Vegetables': df_hist_veg,\n",
    "    'Rice Time Series': df_rice_ts,\n",
    "    'Climate Data': df_climate\n",
    "}\n",
    "\n",
    "summary_table = dataset_summary(datasets)\n",
    "print(\"DATASET STATISTICS OVERVIEW\")\n",
    "print(\"=\" * 80)\n",
    "print(summary_table.to_string(index=False))\n",
    "print(f\"\\nTotal Records: {summary_table['Records'].sum():,}\")\n",
    "print(f\"Total Memory: {summary_table['Memory (KB)'].sum():,.1f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df03cb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3.2 Feature Distribution Analysis\n",
    "# =============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "fig.suptitle('Feature Distribution Analysis Across Datasets', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 1. Paddy - use numeric columns only\n",
    "ax = axes[0, 0]\n",
    "paddy_numeric = df_paddy.select_dtypes(include=[np.number])\n",
    "if len(paddy_numeric.columns) > 0:\n",
    "    first_col = paddy_numeric.columns[0]\n",
    "    data = pd.to_numeric(paddy_numeric[first_col], errors='coerce').dropna()\n",
    "    if len(data) > 0:\n",
    "        data.hist(ax=ax, bins=15, edgecolor='black', alpha=0.7)\n",
    "        ax.axvline(data.mean(), color='red', linestyle='--', label=f'Mean: {data.mean():.0f}')\n",
    "        ax.set_title(f'Paddy: {first_col[:20]}')\n",
    "        ax.legend(fontsize=8)\n",
    "\n",
    "# 2. Rice production over time\n",
    "ax = axes[0, 1]\n",
    "rice_prod_col = [c for c in df_economy.columns if 'rice' in c.lower() and 'production' in c.lower()]\n",
    "if rice_prod_col and 'Year' in df_economy.columns:\n",
    "    df_economy.plot(x='Year', y=rice_prod_col[0], ax=ax, marker='o', legend=False)\n",
    "    ax.set_title('Rice Production Trend')\n",
    "    ax.set_ylabel('Production (Mt)')\n",
    "else:\n",
    "    econ_numeric = df_economy.select_dtypes(include=[np.number])\n",
    "    if len(econ_numeric.columns) > 1:\n",
    "        ax.plot(econ_numeric.iloc[:, 0], econ_numeric.iloc[:, 1], marker='o')\n",
    "        ax.set_title('Economic Trend')\n",
    "\n",
    "# 3. Historical vegetable price distribution  \n",
    "ax = axes[0, 2]\n",
    "hist_numeric = df_hist_veg.select_dtypes(include=[np.number])\n",
    "if len(hist_numeric.columns) > 0:\n",
    "    price_cols = [c for c in hist_numeric.columns if 'price' in c.lower() or 'avg' in c.lower()]\n",
    "    if price_cols:\n",
    "        data = pd.to_numeric(hist_numeric[price_cols[0]], errors='coerce').dropna()\n",
    "    else:\n",
    "        data = pd.to_numeric(hist_numeric.iloc[:, 0], errors='coerce').dropna()\n",
    "    if len(data) > 0:\n",
    "        data.hist(ax=ax, bins=50, edgecolor='black', alpha=0.7, log=True)\n",
    "        ax.set_title('Vegetable Price Distribution (Log)')\n",
    "\n",
    "# 4. Historical vegetable volume distribution\n",
    "ax = axes[1, 0]\n",
    "vol_cols = [c for c in hist_numeric.columns if 'volume' in c.lower() or 'qty' in c.lower()]\n",
    "if vol_cols:\n",
    "    data = pd.to_numeric(hist_numeric[vol_cols[0]], errors='coerce').dropna()\n",
    "    if len(data) > 0:\n",
    "        data.hist(ax=ax, bins=50, edgecolor='black', alpha=0.7, log=True)\n",
    "        ax.set_title('Trading Volume (Log Scale)')\n",
    "\n",
    "# 5. Rice time series - price trend\n",
    "ax = axes[1, 1]\n",
    "rice_numeric = df_rice_ts.select_dtypes(include=[np.number])\n",
    "if len(rice_numeric.columns) > 0:\n",
    "    for i, col in enumerate(rice_numeric.columns[:3]):\n",
    "        data = pd.to_numeric(rice_numeric[col], errors='coerce').dropna()\n",
    "        ax.plot(range(len(data)), data, label=col[:15], alpha=0.7)\n",
    "    ax.set_title('Rice Price Time Series')\n",
    "    ax.legend(fontsize=7)\n",
    "\n",
    "# 6. Climate feature correlation heatmap\n",
    "ax = axes[1, 2]\n",
    "climate_numeric = df_climate.select_dtypes(include=[np.number])\n",
    "if len(climate_numeric.columns) > 1:\n",
    "    # Convert to numeric and compute correlation\n",
    "    climate_clean = climate_numeric.apply(pd.to_numeric, errors='coerce').dropna()\n",
    "    if len(climate_clean) > 0:\n",
    "        corr_climate = climate_clean.corr()\n",
    "        sns.heatmap(corr_climate, ax=ax, cmap='RdBu_r', center=0, annot=True, fmt='.2f', \n",
    "                    annot_kws={'size': 7}, cbar_kws={'shrink': 0.8})\n",
    "        ax.set_title('Climate Features Correlation')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nDistribution Statistics:\")\n",
    "print(\"-\" * 50)\n",
    "for name, df in datasets.items():\n",
    "    numeric = df.select_dtypes(include=[np.number]).apply(pd.to_numeric, errors='coerce')\n",
    "    if len(numeric.columns) > 0:\n",
    "        skew = numeric.skew().abs().mean()\n",
    "        if not np.isnan(skew):\n",
    "            print(f\"{name}: Mean Skewness = {skew:.3f} {'(High)' if skew > 1 else '(Normal)'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1418ef51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3.3 Regional & Temporal Coverage Analysis\n",
    "# =============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Data Coverage Analysis: Regional & Temporal', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 1. Regional coverage in paddy data\n",
    "ax = axes[0, 0]\n",
    "region_col = [c for c in df_paddy.columns if 'region' in c.lower() or 'district' in c.lower()]\n",
    "if region_col:\n",
    "    region_counts = df_paddy[region_col[0]].value_counts()\n",
    "    region_counts.plot(kind='barh', ax=ax, color=plt.cm.viridis(np.linspace(0, 1, len(region_counts))))\n",
    "    ax.set_xlabel('Number of Records')\n",
    "    ax.set_title(f'Regional Coverage: Paddy Data ({len(region_counts)} regions)')\n",
    "    ax.axvline(region_counts.mean(), color='red', linestyle='--', label=f'Mean: {region_counts.mean():.0f}')\n",
    "\n",
    "# 2. Temporal coverage\n",
    "ax = axes[0, 1]\n",
    "temporal_data = []\n",
    "\n",
    "# Check each dataset for temporal columns\n",
    "for name, df in datasets.items():\n",
    "    year_cols = [c for c in df.columns if 'year' in c.lower()]\n",
    "    date_cols = [c for c in df.columns if 'date' in c.lower()]\n",
    "    \n",
    "    if year_cols:\n",
    "        years = pd.to_numeric(df[year_cols[0]], errors='coerce').dropna()\n",
    "        if len(years) > 0:\n",
    "            temporal_data.append({\n",
    "                'Dataset': name[:15],\n",
    "                'Min Year': int(years.min()),\n",
    "                'Max Year': int(years.max()),\n",
    "                'Span': int(years.max() - years.min())\n",
    "            })\n",
    "\n",
    "if temporal_data:\n",
    "    temp_df = pd.DataFrame(temporal_data)\n",
    "    y_pos = range(len(temp_df))\n",
    "    ax.barh(y_pos, temp_df['Span'], left=temp_df['Min Year'], height=0.6, alpha=0.7)\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(temp_df['Dataset'])\n",
    "    ax.set_xlabel('Year')\n",
    "    ax.set_title('Temporal Coverage by Dataset')\n",
    "    for i, row in temp_df.iterrows():\n",
    "        ax.text(row['Min Year'] + row['Span']/2, i, f\"{row['Min Year']}-{row['Max Year']}\", \n",
    "                ha='center', va='center', fontsize=8, fontweight='bold')\n",
    "\n",
    "# 3. Crop distribution in historical data\n",
    "ax = axes[1, 0]\n",
    "crop_col = [c for c in df_hist_veg.columns if 'crop' in c.lower() or 'commodity' in c.lower() or 'item' in c.lower()]\n",
    "if crop_col:\n",
    "    crop_counts = df_hist_veg[crop_col[0]].value_counts().head(15)\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(crop_counts)))\n",
    "    crop_counts.plot(kind='bar', ax=ax, color=colors, edgecolor='black')\n",
    "    ax.set_title(f'Top 15 Crops by Record Count (Total: {df_hist_veg[crop_col[0]].nunique()})')\n",
    "    ax.set_xlabel('Crop')\n",
    "    ax.set_ylabel('Records')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Calculate imbalance ratio\n",
    "    imbalance = crop_counts.max() / crop_counts.min()\n",
    "    ax.text(0.95, 0.95, f'Imbalance Ratio: {imbalance:.1f}x', transform=ax.transAxes,\n",
    "            fontsize=9, ha='right', va='top', bbox=dict(boxstyle='round', facecolor='wheat'))\n",
    "\n",
    "# 4. Missing data heatmap\n",
    "ax = axes[1, 1]\n",
    "missing_pct = pd.DataFrame({name: (df.isnull().sum() / len(df) * 100).head(10) \n",
    "                            for name, df in datasets.items()})\n",
    "if missing_pct.sum().sum() > 0:\n",
    "    sns.heatmap(missing_pct.T, ax=ax, cmap='YlOrRd', annot=True, fmt='.1f',\n",
    "                cbar_kws={'label': 'Missing %'})\n",
    "    ax.set_title('Missing Data Analysis (Top 10 Features)')\n",
    "else:\n",
    "    ax.text(0.5, 0.5, 'No Missing Data\\nAll datasets complete', ha='center', va='center',\n",
    "            fontsize=14, transform=ax.transAxes)\n",
    "    ax.set_title('Missing Data Analysis')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c2f943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3.4 Feature Variance & Outlier Analysis\n",
    "# =============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Feature Variance & Outlier Detection', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 1. Coefficient of Variation across datasets\n",
    "ax = axes[0, 0]\n",
    "cv_data = {}\n",
    "for name, df in datasets.items():\n",
    "    numeric = df.select_dtypes(include=[np.number])\n",
    "    if len(numeric.columns) > 0:\n",
    "        cv = (numeric.std() / numeric.mean().replace(0, np.nan)).dropna()\n",
    "        cv_data[name[:12]] = cv.mean()\n",
    "\n",
    "if cv_data:\n",
    "    bars = ax.bar(cv_data.keys(), cv_data.values(), color=plt.cm.coolwarm(np.linspace(0.3, 0.7, len(cv_data))))\n",
    "    ax.set_ylabel('Coefficient of Variation')\n",
    "    ax.set_title('Data Variability by Dataset')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    ax.axhline(y=1.0, color='red', linestyle='--', label='CV=1 threshold')\n",
    "    ax.legend()\n",
    "\n",
    "# 2. Outlier detection using IQR method\n",
    "ax = axes[0, 1]\n",
    "outlier_pct = {}\n",
    "for name, df in datasets.items():\n",
    "    numeric = df.select_dtypes(include=[np.number])\n",
    "    total_outliers = 0\n",
    "    total_values = 0\n",
    "    for col in numeric.columns:\n",
    "        Q1, Q3 = numeric[col].quantile([0.25, 0.75])\n",
    "        IQR = Q3 - Q1\n",
    "        outliers = ((numeric[col] < Q1 - 1.5*IQR) | (numeric[col] > Q3 + 1.5*IQR)).sum()\n",
    "        total_outliers += outliers\n",
    "        total_values += len(numeric[col].dropna())\n",
    "    if total_values > 0:\n",
    "        outlier_pct[name[:12]] = total_outliers / total_values * 100\n",
    "\n",
    "if outlier_pct:\n",
    "    colors = ['#ff6b6b' if v > 5 else '#4ecdc4' for v in outlier_pct.values()]\n",
    "    ax.barh(list(outlier_pct.keys()), list(outlier_pct.values()), color=colors)\n",
    "    ax.set_xlabel('Outlier Percentage (%)')\n",
    "    ax.set_title('Outlier Prevalence (IQR Method)')\n",
    "    ax.axvline(x=5, color='red', linestyle='--', label='5% threshold')\n",
    "    ax.legend()\n",
    "\n",
    "# 3. Box plots for key features\n",
    "ax = axes[1, 0]\n",
    "key_features = []\n",
    "if 'Yield_kg_per_ha' in df_paddy.columns:\n",
    "    key_features.append(('Paddy Yield', df_paddy['Yield_kg_per_ha']))\n",
    "if 'Avg_Price' in df_hist_veg.columns:\n",
    "    key_features.append(('Veg Price', df_hist_veg['Avg_Price']))\n",
    "elif len(df_hist_veg.select_dtypes(include=[np.number]).columns) > 0:\n",
    "    price_col = df_hist_veg.select_dtypes(include=[np.number]).columns[0]\n",
    "    key_features.append(('Veg Price', df_hist_veg[price_col]))\n",
    "\n",
    "if key_features:\n",
    "    box_data = [f[1].dropna().values[:1000] for f in key_features]  # Sample for performance\n",
    "    bp = ax.boxplot(box_data, labels=[f[0] for f in key_features], patch_artist=True)\n",
    "    colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
    "    for patch, color in zip(bp['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.7)\n",
    "    ax.set_title('Key Feature Distributions')\n",
    "    ax.set_ylabel('Value')\n",
    "\n",
    "# 4. Skewness distribution\n",
    "ax = axes[1, 1]\n",
    "all_skewness = []\n",
    "skew_labels = []\n",
    "for name, df in datasets.items():\n",
    "    numeric = df.select_dtypes(include=[np.number])\n",
    "    for col in numeric.columns:\n",
    "        skew_val = numeric[col].skew()\n",
    "        if not np.isnan(skew_val):\n",
    "            all_skewness.append(skew_val)\n",
    "            skew_labels.append(f\"{name[:8]}: {col[:10]}\")\n",
    "\n",
    "if all_skewness:\n",
    "    sorted_idx = np.argsort(np.abs(all_skewness))[::-1][:15]  # Top 15 most skewed\n",
    "    colors = ['#e74c3c' if abs(all_skewness[i]) > 1 else '#3498db' for i in sorted_idx]\n",
    "    ax.barh([skew_labels[i] for i in sorted_idx], [all_skewness[i] for i in sorted_idx], color=colors)\n",
    "    ax.axvline(x=0, color='black', linestyle='-')\n",
    "    ax.axvline(x=-1, color='red', linestyle='--', alpha=0.5)\n",
    "    ax.axvline(x=1, color='red', linestyle='--', alpha=0.5)\n",
    "    ax.set_xlabel('Skewness')\n",
    "    ax.set_title('Top 15 Most Skewed Features')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nVariance Analysis Summary:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Datasets with high variability (CV > 1): {sum(1 for v in cv_data.values() if v > 1)}/{len(cv_data)}\")\n",
    "print(f\"Datasets with >5% outliers: {sum(1 for v in outlier_pct.values() if v > 5)}/{len(outlier_pct)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d7ac2b",
   "metadata": {},
   "source": [
    "## 5. Dataset-to-Model Mapping\n",
    "\n",
    "This section illustrates how each dataset contributes to the ACA-O system components and their interconnections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e755fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 4.1 Dataset-Model Relationship Visualization\n",
    "# =============================================================================\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 10))\n",
    "ax.set_xlim(0, 100)\n",
    "ax.set_ylim(0, 70)\n",
    "ax.axis('off')\n",
    "ax.set_title('ACA-O System: Dataset to Model Mapping', fontsize=16, fontweight='bold', pad=20)\n",
    "\n",
    "# Define positions\n",
    "datasets_pos = [(10, 60), (10, 45), (10, 30), (10, 15)]\n",
    "models_pos = [(50, 55), (50, 40), (50, 25)]\n",
    "output_pos = [(90, 40)]\n",
    "\n",
    "# Draw datasets (left side)\n",
    "dataset_info = [\n",
    "    ('Paddy Statistics', '28 records', '#3498db'),\n",
    "    ('Climate Data', '1800 records', '#2ecc71'),\n",
    "    ('Historical Veg', '130K records', '#e74c3c'),\n",
    "    ('Rice Time Series', '324 records', '#9b59b6')\n",
    "]\n",
    "\n",
    "for (x, y), (name, count, color) in zip(datasets_pos, dataset_info):\n",
    "    rect = plt.Rectangle((x-8, y-4), 20, 8, facecolor=color, alpha=0.3, edgecolor=color, linewidth=2)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(x+2, y+1, name, fontsize=9, fontweight='bold', ha='center')\n",
    "    ax.text(x+2, y-2, count, fontsize=8, ha='center', style='italic')\n",
    "\n",
    "# Draw models (middle)\n",
    "model_info = [\n",
    "    ('CropRecommender', 'Embedding NN', '#ff6b6b'),\n",
    "    ('YieldPredictor', 'MLP Regressor', '#4ecdc4'),\n",
    "    ('PriceLSTM', 'Seq2Seq', '#45b7d1')\n",
    "]\n",
    "\n",
    "for (x, y), (name, arch, color) in zip(models_pos, model_info):\n",
    "    ellipse = plt.Circle((x, y), 8, facecolor=color, alpha=0.3, edgecolor=color, linewidth=2)\n",
    "    ax.add_patch(ellipse)\n",
    "    ax.text(x, y+2, name, fontsize=9, fontweight='bold', ha='center')\n",
    "    ax.text(x, y-2, arch, fontsize=8, ha='center', style='italic')\n",
    "\n",
    "# Draw output (right side)\n",
    "rect = plt.Rectangle((82, 32), 16, 16, facecolor='#f39c12', alpha=0.3, edgecolor='#f39c12', linewidth=2)\n",
    "ax.add_patch(rect)\n",
    "ax.text(90, 42, 'Area', fontsize=10, fontweight='bold', ha='center')\n",
    "ax.text(90, 38, 'Optimizer', fontsize=10, fontweight='bold', ha='center')\n",
    "ax.text(90, 34, '(Linear Prog)', fontsize=8, ha='center', style='italic')\n",
    "\n",
    "# Draw connections\n",
    "connections = [\n",
    "    ((22, 60), (42, 55), '#3498db'),  # Paddy -> CropRec\n",
    "    ((22, 45), (42, 55), '#2ecc71'),  # Climate -> CropRec\n",
    "    ((22, 45), (42, 40), '#2ecc71'),  # Climate -> Yield\n",
    "    ((22, 30), (42, 40), '#e74c3c'),  # HistVeg -> Yield\n",
    "    ((22, 15), (42, 25), '#9b59b6'),  # RiceTS -> Price\n",
    "    ((22, 30), (42, 25), '#e74c3c'),  # HistVeg -> Price\n",
    "    ((58, 55), (82, 42), '#ff6b6b'),  # CropRec -> Optimizer\n",
    "    ((58, 40), (82, 40), '#4ecdc4'),  # Yield -> Optimizer\n",
    "    ((58, 25), (82, 38), '#45b7d1'),  # Price -> Optimizer\n",
    "]\n",
    "\n",
    "for (x1, y1), (x2, y2), color in connections:\n",
    "    ax.annotate('', xy=(x2, y2), xytext=(x1, y1),\n",
    "                arrowprops=dict(arrowstyle='->', color=color, lw=1.5, alpha=0.7))\n",
    "\n",
    "# Add legend\n",
    "ax.text(50, 5, 'Data Flow: Datasets â†’ Models â†’ Optimization Output', \n",
    "        fontsize=10, ha='center', style='italic', color='gray')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print feature mapping table\n",
    "print(\"\\nDATASET-MODEL FEATURE MAPPING\")\n",
    "print(\"=\" * 80)\n",
    "mapping = [\n",
    "    ['Paddy Statistics', 'CropRecommender', 'Region, Season, Crop â†’ Embedding vectors'],\n",
    "    ['Climate Data', 'CropRecommender', 'Temp, Humidity, Rainfall â†’ Climate features'],\n",
    "    ['Climate Data', 'YieldPredictor', 'Climate conditions â†’ Yield estimation'],\n",
    "    ['Historical Veg', 'YieldPredictor', 'Historical yields â†’ Training targets'],\n",
    "    ['Historical Veg', 'PriceLSTM', 'Price sequences â†’ Pattern learning'],\n",
    "    ['Rice Time Series', 'PriceLSTM', 'Monthly prices â†’ Price forecasting'],\n",
    "    ['All Models', 'AreaOptimizer', 'Predictions â†’ Linear programming constraints']\n",
    "]\n",
    "for src, tgt, desc in mapping:\n",
    "    print(f\"{src:20} â†’ {tgt:18} | {desc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0cfd32",
   "metadata": {},
   "source": [
    "## 6. Economy-Rice Production Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1ffd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find target column dynamically\n",
    "target_candidates = [c for c in df_economy.columns if 'rice' in c.lower() and 'production' in c.lower()]\n",
    "ECON_TARGET_COL = target_candidates[0] if target_candidates else df_economy.columns[-1]\n",
    "print(f\"Target: {ECON_TARGET_COL}\")\n",
    "\n",
    "numeric_cols = df_economy.select_dtypes(include=[np.number]).columns.tolist()\n",
    "ECON_FEATURE_COLS = [c for c in numeric_cols if c != ECON_TARGET_COL]\n",
    "\n",
    "econ_df = df_economy.dropna(subset=[ECON_TARGET_COL] + ECON_FEATURE_COLS).copy()\n",
    "X_econ = econ_df[ECON_FEATURE_COLS].values.astype(np.float32)\n",
    "y_econ = econ_df[ECON_TARGET_COL].values.astype(np.float32).reshape(-1, 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_econ, y_econ, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler_econ = StandardScaler()\n",
    "X_train_scaled = scaler_econ.fit_transform(X_train)\n",
    "X_test_scaled = scaler_econ.transform(X_test)\n",
    "\n",
    "lr = LinearRegression().fit(X_train_scaled, y_train)\n",
    "print(f'Linear Regression RMSE: {np.sqrt(mean_squared_error(y_test, lr.predict(X_test_scaled))):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e795f54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features, hidden=(64, 32)):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for h in hidden:\n",
    "            layers.extend([nn.Linear(in_features, h), nn.ReLU()])\n",
    "            in_features = h\n",
    "        layers.append(nn.Linear(in_features, 1))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "mlp = MLP(len(ECON_FEATURE_COLS)).to(DEVICE)\n",
    "opt = torch.optim.Adam(mlp.parameters(), lr=1e-3)\n",
    "crit = nn.MSELoss()\n",
    "\n",
    "X_t = torch.from_numpy(X_train_scaled).to(DEVICE)\n",
    "y_t = torch.from_numpy(y_train).to(DEVICE)\n",
    "\n",
    "for epoch in range(500):\n",
    "    mlp.train()\n",
    "    opt.zero_grad()\n",
    "    loss = crit(mlp(X_t), y_t)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "mlp.eval()\n",
    "with torch.no_grad():\n",
    "    pred = mlp(torch.from_numpy(X_test_scaled).to(DEVICE)).cpu().numpy()\n",
    "print(f'MLP RMSE: {np.sqrt(mean_squared_error(y_test, pred)):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4299277f",
   "metadata": {},
   "source": [
    "## 7. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4605fce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Historical vegetable/fruit prices with climate\n",
    "df_hist = df_hist_veg.rename(columns={\n",
    "    'Date': 'date', 'Region': 'region',\n",
    "    'Temperature (Â°C)': 'temperature', 'Rainfall (mm)': 'rainfall',\n",
    "    'Humidity (%)': 'humidity', 'Crop Yield Impact Score': 'yield_impact_score',\n",
    "    'fruit_Commodity': 'fruit_type', 'fruit_Price per Unit (LKR/kg)': 'fruit_price',\n",
    "    'vegitable_Commodity': 'vegetable_type', 'vegitable_Price per Unit (LKR/kg)': 'vegetable_price'\n",
    "})\n",
    "df_hist['date'] = pd.to_datetime(df_hist['date'], errors='coerce')\n",
    "\n",
    "# Convert numeric columns\n",
    "for col in ['temperature', 'rainfall', 'humidity', 'yield_impact_score', 'vegetable_price', 'fruit_price']:\n",
    "    df_hist[col] = pd.to_numeric(df_hist[col], errors='coerce')\n",
    "\n",
    "# Rice time series\n",
    "df_rice = df_rice_ts.copy()\n",
    "df_rice['date'] = pd.to_datetime(df_rice['date'], errors='coerce')\n",
    "df_rice = df_rice[['date', 'price', 'production', 'production_total', 'exchange_rate', 'fuel_price']].dropna()\n",
    "\n",
    "print(f\"Historical data: {len(df_hist)} rows, {df_hist['region'].nunique()} regions, \"\n",
    "      f\"{df_hist['vegetable_type'].nunique()} vegetables\")\n",
    "print(f\"Rice data: {len(df_rice)} rows, date range: {df_rice['date'].min()} to {df_rice['date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8957d5",
   "metadata": {},
   "source": [
    "## 8. Climate-Yield Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8f7d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Climate-yield correlation\n",
    "climate_cols = ['temperature', 'rainfall', 'humidity', 'yield_impact_score']\n",
    "df_climate_yield = df_hist[climate_cols].dropna()\n",
    "\n",
    "corr = df_climate_yield.corr()\n",
    "print(\"Correlation with yield_impact_score:\")\n",
    "print(corr['yield_impact_score'].sort_values(ascending=False))\n",
    "\n",
    "# Regional aggregation\n",
    "regional = df_hist.groupby('region').agg({\n",
    "    'temperature': 'mean', 'rainfall': 'mean', 'humidity': 'mean',\n",
    "    'yield_impact_score': 'mean', 'vegetable_price': 'mean', 'fruit_price': 'mean'\n",
    "}).round(2).sort_values('yield_impact_score', ascending=False)\n",
    "\n",
    "display(regional.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f954f733",
   "metadata": {},
   "source": [
    "## 9. Crop Recommendation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dd755e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Prepare crop recommendation data\n",
    "df_crop = df_hist[['region', 'date', 'temperature', 'rainfall', 'humidity', \n",
    "                   'yield_impact_score', 'vegetable_type']].dropna()\n",
    "df_crop['month'] = df_crop['date'].dt.month\n",
    "\n",
    "# Encoders\n",
    "region_enc = LabelEncoder()\n",
    "crop_enc = LabelEncoder()\n",
    "df_crop['region_idx'] = region_enc.fit_transform(df_crop['region'])\n",
    "df_crop['crop_idx'] = crop_enc.fit_transform(df_crop['vegetable_type'])\n",
    "\n",
    "n_regions = len(region_enc.classes_)\n",
    "n_crops = len(crop_enc.classes_)\n",
    "\n",
    "# Best crop per region-month\n",
    "best_crops = df_crop.groupby(['region', 'month', 'vegetable_type']).agg({\n",
    "    'yield_impact_score': 'mean', 'temperature': 'mean', \n",
    "    'rainfall': 'mean', 'humidity': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "best_crops = best_crops.loc[best_crops.groupby(['region', 'month'])['yield_impact_score'].idxmax()]\n",
    "best_crops['region_idx'] = region_enc.transform(best_crops['region'])\n",
    "best_crops['crop_idx'] = crop_enc.transform(best_crops['vegetable_type'])\n",
    "\n",
    "print(f\"Regions: {n_regions}, Crops: {n_crops}, Samples: {len(best_crops)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7640b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CropRecommender(nn.Module):\n",
    "    def __init__(self, n_regions, n_crops, emb_dim=16):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(n_regions, emb_dim)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(emb_dim + 4, 64), nn.ReLU(), nn.BatchNorm1d(64), nn.Dropout(0.3),\n",
    "            nn.Linear(64, 32), nn.ReLU(), nn.BatchNorm1d(32), nn.Dropout(0.2),\n",
    "            nn.Linear(32, n_crops)\n",
    "        )\n",
    "    \n",
    "    def forward(self, region, climate):\n",
    "        x = torch.cat([self.emb(region), climate], dim=1)\n",
    "        return self.net(x)\n",
    "\n",
    "# Prepare data\n",
    "X_region = best_crops['region_idx'].values\n",
    "X_climate = best_crops[['temperature', 'rainfall', 'humidity', 'month']].values.astype(np.float32)\n",
    "y = best_crops['crop_idx'].values\n",
    "\n",
    "climate_scaler = StandardScaler()\n",
    "X_climate_scaled = climate_scaler.fit_transform(X_climate)\n",
    "\n",
    "X_r_train, X_r_test, X_c_train, X_c_test, y_train, y_test = train_test_split(\n",
    "    X_region, X_climate_scaled, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Convert to tensors\n",
    "X_r_train_t = torch.from_numpy(X_r_train).long().to(DEVICE)\n",
    "X_c_train_t = torch.from_numpy(X_c_train).float().to(DEVICE)\n",
    "y_train_t = torch.from_numpy(y_train).long().to(DEVICE)\n",
    "X_r_test_t = torch.from_numpy(X_r_test).long().to(DEVICE)\n",
    "X_c_test_t = torch.from_numpy(X_c_test).float().to(DEVICE)\n",
    "y_test_t = torch.from_numpy(y_test).long().to(DEVICE)\n",
    "\n",
    "print(f\"Train: {len(y_train)}, Test: {len(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7512fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train crop recommender\n",
    "crop_model = CropRecommender(n_regions, n_crops).to(DEVICE)\n",
    "opt_crop = torch.optim.Adam(crop_model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "crit_crop = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(100):\n",
    "    crop_model.train()\n",
    "    opt_crop.zero_grad()\n",
    "    out = crop_model(X_r_train_t, X_c_train_t)\n",
    "    loss = crit_crop(out, y_train_t)\n",
    "    loss.backward()\n",
    "    opt_crop.step()\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        crop_model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred = crop_model(X_r_test_t, X_c_test_t).argmax(1)\n",
    "            acc = (pred == y_test_t).float().mean()\n",
    "        print(f'Epoch {epoch+1}: Loss={loss.item():.4f}, Acc={acc:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ab0806",
   "metadata": {},
   "source": [
    "## 10. Yield Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c54dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YieldPredictor(nn.Module):\n",
    "    def __init__(self, n_regions, n_crops, emb_dim=8):\n",
    "        super().__init__()\n",
    "        self.region_emb = nn.Embedding(n_regions, emb_dim)\n",
    "        self.crop_emb = nn.Embedding(n_crops, emb_dim)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(emb_dim * 2 + 4, 64), nn.ReLU(), nn.BatchNorm1d(64), nn.Dropout(0.3),\n",
    "            nn.Linear(64, 32), nn.ReLU(), nn.Linear(32, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, region, crop, climate):\n",
    "        x = torch.cat([self.region_emb(region), self.crop_emb(crop), climate], dim=1)\n",
    "        return self.net(x)\n",
    "\n",
    "# Prepare yield data (sample for speed)\n",
    "df_yield = df_hist[['region', 'vegetable_type', 'temperature', 'rainfall', \n",
    "                    'humidity', 'yield_impact_score', 'date']].dropna()\n",
    "df_yield['month'] = df_yield['date'].dt.month\n",
    "df_yield['region_idx'] = region_enc.transform(df_yield['region'])\n",
    "df_yield['crop_idx'] = crop_enc.transform(df_yield['vegetable_type'])\n",
    "df_yield = df_yield.sample(n=min(50000, len(df_yield)), random_state=42)\n",
    "\n",
    "X_r_y = df_yield['region_idx'].values\n",
    "X_c_y = df_yield['crop_idx'].values\n",
    "X_clim_y = df_yield[['temperature', 'rainfall', 'humidity', 'month']].values.astype(np.float32)\n",
    "y_yield = df_yield['yield_impact_score'].values.astype(np.float32).reshape(-1, 1)\n",
    "\n",
    "X_clim_y_scaled = climate_scaler.transform(X_clim_y)\n",
    "yield_scaler = StandardScaler()\n",
    "y_yield_scaled = yield_scaler.fit_transform(y_yield)\n",
    "\n",
    "(X_r_y_train, X_r_y_test, X_c_y_train, X_c_y_test, \n",
    " X_clim_y_train, X_clim_y_test, y_y_train, y_y_test) = train_test_split(\n",
    "    X_r_y, X_c_y, X_clim_y_scaled, y_yield_scaled, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Yield prediction samples: {len(y_y_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ac3531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train yield predictor\n",
    "yield_model = YieldPredictor(n_regions, n_crops).to(DEVICE)\n",
    "opt_yield = torch.optim.Adam(yield_model.parameters(), lr=1e-3)\n",
    "crit_yield = nn.MSELoss()\n",
    "\n",
    "# Tensors\n",
    "X_r_y_t = torch.from_numpy(X_r_y_train).long().to(DEVICE)\n",
    "X_c_y_t = torch.from_numpy(X_c_y_train).long().to(DEVICE)\n",
    "X_clim_y_t = torch.from_numpy(X_clim_y_train).float().to(DEVICE)\n",
    "y_y_t = torch.from_numpy(y_y_train).float().to(DEVICE)\n",
    "\n",
    "for epoch in range(50):\n",
    "    yield_model.train()\n",
    "    idx = np.random.permutation(len(y_y_t))\n",
    "    \n",
    "    for start in range(0, len(idx), 256):\n",
    "        batch = idx[start:start+256]\n",
    "        opt_yield.zero_grad()\n",
    "        out = yield_model(X_r_y_t[batch], X_c_y_t[batch], X_clim_y_t[batch])\n",
    "        loss = crit_yield(out, y_y_t[batch])\n",
    "        loss.backward()\n",
    "        opt_yield.step()\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}: Loss={loss.item():.4f}')\n",
    "\n",
    "# Evaluate\n",
    "yield_model.eval()\n",
    "with torch.no_grad():\n",
    "    pred = yield_model(\n",
    "        torch.from_numpy(X_r_y_test).long().to(DEVICE),\n",
    "        torch.from_numpy(X_c_y_test).long().to(DEVICE),\n",
    "        torch.from_numpy(X_clim_y_test).float().to(DEVICE)\n",
    "    ).cpu().numpy()\n",
    "    pred_orig = yield_scaler.inverse_transform(pred)\n",
    "    y_orig = yield_scaler.inverse_transform(y_y_test)\n",
    "    \n",
    "print(f'Yield RMSE: {np.sqrt(mean_squared_error(y_orig, pred_orig)):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533f1efd",
   "metadata": {},
   "source": [
    "## 11. Area Allocation Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ad6bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import linprog\n",
    "\n",
    "def predict_yield_single(region, crop, temp, rain, humid, month):\n",
    "    yield_model.eval()\n",
    "    try:\n",
    "        r_idx = region_enc.transform([region])[0]\n",
    "        c_idx = crop_enc.transform([crop])[0]\n",
    "    except ValueError:\n",
    "        return 1.0\n",
    "    \n",
    "    clim = climate_scaler.transform([[temp, rain, humid, month]])\n",
    "    with torch.no_grad():\n",
    "        pred = yield_model(\n",
    "            torch.tensor([r_idx]).long().to(DEVICE),\n",
    "            torch.tensor([c_idx]).long().to(DEVICE),\n",
    "            torch.from_numpy(clim).float().to(DEVICE)\n",
    "        ).cpu().numpy()\n",
    "    return float(yield_scaler.inverse_transform(pred)[0, 0])\n",
    "\n",
    "class AreaOptimizer:\n",
    "    def __init__(self, crops, region, total_area):\n",
    "        self.crops = crops\n",
    "        self.region = region\n",
    "        self.total_area = total_area\n",
    "    \n",
    "    def get_prices(self):\n",
    "        return np.array([\n",
    "            df_hist[df_hist['vegetable_type'] == c]['vegetable_price'].mean() or 100\n",
    "            for c in self.crops\n",
    "        ])\n",
    "    \n",
    "    def optimize(self, temp, rain, humid, month, min_div=0.05):\n",
    "        yields = np.array([predict_yield_single(self.region, c, temp, rain, humid, month) \n",
    "                          for c in self.crops])\n",
    "        yields = np.maximum(yields, 0.1)\n",
    "        prices = self.get_prices()\n",
    "        \n",
    "        c = -(yields * prices)\n",
    "        A_eq = np.ones((1, len(self.crops)))\n",
    "        b_eq = np.array([self.total_area])\n",
    "        bounds = [(min_div * self.total_area, \n",
    "                   (1 - min_div * (len(self.crops) - 1)) * self.total_area) \n",
    "                  for _ in self.crops]\n",
    "        \n",
    "        result = linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method='highs')\n",
    "        alloc = result.x if result.success else np.full(len(self.crops), self.total_area / len(self.crops))\n",
    "        \n",
    "        return {\n",
    "            'crops': self.crops,\n",
    "            'allocation_ha': alloc.round(1).tolist(),\n",
    "            'allocation_pct': (alloc / self.total_area * 100).round(1).tolist(),\n",
    "            'expected_revenue': float(np.sum(alloc * yields * prices))\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1264c042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test optimizer\n",
    "top_crops = df_hist['vegetable_type'].value_counts().head(5).index.tolist()\n",
    "opt = AreaOptimizer(top_crops, 'Anuradhapura', 100)\n",
    "\n",
    "for name, params in [\n",
    "    ('Maha', {'temp': 28, 'rain': 250, 'humid': 85, 'month': 11}),\n",
    "    ('Yala', {'temp': 33, 'rain': 50, 'humid': 60, 'month': 5})\n",
    "]:\n",
    "    result = opt.optimize(**params)\n",
    "    print(f\"\\n{name} Season:\")\n",
    "    for c, a, p in zip(result['crops'], result['allocation_ha'], result['allocation_pct']):\n",
    "        print(f\"  {c}: {a} ha ({p}%)\")\n",
    "    print(f\"  Revenue: LKR {result['expected_revenue']:,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c2fbab",
   "metadata": {},
   "source": [
    "## 11.1 Price Forecasting (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abaa2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Price LSTM Model Definition\n",
    "# =============================================================================\n",
    "\n",
    "class PriceLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden=64, layers=2, horizon=6):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden, layers, batch_first=True, dropout=0.2)\n",
    "        self.fc = nn.Sequential(nn.Linear(hidden, 32), nn.ReLU(), nn.Linear(32, horizon))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        return self.fc(out[:, -1, :])\n",
    "\n",
    "# Prepare data\n",
    "WINDOW, HORIZON = 30, 6\n",
    "feature_cols = ['price', 'production', 'exchange_rate', 'fuel_price']\n",
    "features = df_rice[feature_cols].values.astype(np.float32)\n",
    "\n",
    "price_scaler = MinMaxScaler()\n",
    "features_scaled = price_scaler.fit_transform(features)\n",
    "\n",
    "def make_sequences(data, window, horizon):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(data) - window - horizon + 1):\n",
    "        X.append(data[i:i+window])\n",
    "        Y.append(data[i+window:i+window+horizon, 0])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "X_price, Y_price = make_sequences(features_scaled, WINDOW, HORIZON)\n",
    "split = int(len(X_price) * 0.8)\n",
    "\n",
    "X_train_p = torch.from_numpy(X_price[:split]).float().to(DEVICE)\n",
    "Y_train_p = torch.from_numpy(Y_price[:split]).float().to(DEVICE)\n",
    "X_test_p = torch.from_numpy(X_price[split:]).float().to(DEVICE)\n",
    "Y_test_p = torch.from_numpy(Y_price[split:]).float().to(DEVICE)\n",
    "\n",
    "print(f\"Sequences: X={X_price.shape}, Train={len(X_train_p)}, Test={len(X_test_p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e842650c",
   "metadata": {},
   "source": [
    "## 11.2 Advanced Area Allocation Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083bf016",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import linprog\n",
    "\n",
    "def predict_yield_for_crop(region, crop, temperature, rainfall, humidity, month):\n",
    "    yield_model.eval()\n",
    "    try:\n",
    "        r_idx = region_enc.transform([region])[0]\n",
    "        c_idx = crop_enc.transform([crop])[0]\n",
    "    except ValueError:\n",
    "        return 1.0\n",
    "    \n",
    "    climate = climate_scaler.transform([[temperature, rainfall, humidity, month]])\n",
    "    with torch.no_grad():\n",
    "        pred = yield_model(\n",
    "            torch.tensor([r_idx]).long().to(DEVICE),\n",
    "            torch.tensor([c_idx]).long().to(DEVICE),\n",
    "            torch.from_numpy(climate).float().to(DEVICE)\n",
    "        )\n",
    "        return float(yield_scaler.inverse_transform(pred.cpu().numpy())[0, 0])\n",
    "\n",
    "class AreaOptimizer:\n",
    "    def __init__(self, crops, region, total_area):\n",
    "        self.crops = crops\n",
    "        self.region = region\n",
    "        self.total_area = total_area\n",
    "        self.n_crops = len(crops)\n",
    "    \n",
    "    def get_yields(self, temp, rain, hum, month):\n",
    "        return np.array([max(predict_yield_for_crop(self.region, c, temp, rain, hum, month), 0.1) for c in self.crops])\n",
    "    \n",
    "    def get_prices(self):\n",
    "        prices = []\n",
    "        for crop in self.crops:\n",
    "            data = df_hist[df_hist['vegetable_type'] == crop]\n",
    "            prices.append(data['vegetable_price'].mean() if len(data) > 0 else 100)\n",
    "        return np.array(prices)\n",
    "    \n",
    "    def optimize(self, temp, rain, hum, month, min_div=0.05):\n",
    "        yields = self.get_yields(temp, rain, hum, month)\n",
    "        prices = self.get_prices()\n",
    "        \n",
    "        c = -(yields * prices)\n",
    "        A_eq = np.ones((1, self.n_crops))\n",
    "        b_eq = np.array([self.total_area])\n",
    "        bounds = [(min_div * self.total_area, (1 - min_div * (self.n_crops - 1)) * self.total_area)] * self.n_crops\n",
    "        \n",
    "        result = linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method='highs')\n",
    "        alloc = result.x if result.success else np.full(self.n_crops, self.total_area / self.n_crops)\n",
    "        \n",
    "        return {\n",
    "            'crops': self.crops,\n",
    "            'allocations_ha': alloc.tolist(),\n",
    "            'allocation_pct': (alloc / self.total_area * 100).tolist(),\n",
    "            'expected_yields': yields.tolist(),\n",
    "            'expected_prices': prices.tolist(),\n",
    "            'total_expected_revenue': np.sum(alloc * yields * prices),\n",
    "            'success': result.success if hasattr(result, 'success') else True\n",
    "        }\n",
    "\n",
    "print(\"AreaOptimizer defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7c7265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test area optimizer\n",
    "available_crops = df_hist['vegetable_type'].value_counts().head(5).index.tolist()\n",
    "print(f\"Crops: {available_crops}\")\n",
    "\n",
    "optimizer = AreaOptimizer(crops=available_crops, region='Anuradhapura', total_area=100)\n",
    "\n",
    "scenarios = [\n",
    "    (\"Wet Season\", 28, 250, 85, 11),\n",
    "    (\"Dry Season\", 33, 50, 60, 5),\n",
    "    (\"Inter-monsoon\", 30, 150, 75, 3),\n",
    "]\n",
    "\n",
    "for name, temp, rain, hum, month in scenarios:\n",
    "    result = optimizer.optimize(temp, rain, hum, month)\n",
    "    print(f\"\\n{name}: {temp}C, {rain}mm, {hum}%\")\n",
    "    for c, a, p in zip(result['crops'], result['allocations_ha'], result['allocation_pct']):\n",
    "        print(f\"  {c}: {a:.1f}ha ({p:.1f}%)\")\n",
    "    print(f\"  Revenue: LKR {result['total_expected_revenue']:,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b79515",
   "metadata": {},
   "source": [
    "## 12. Model Generalization & Cross-Validation Analysis\n",
    "\n",
    "This section evaluates model generalization capabilities through:\n",
    "- K-Fold Cross-Validation for standard models\n",
    "- Time Series Cross-Validation for temporal models\n",
    "- Learning curve analysis\n",
    "- Performance stability assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a640fdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 11.1 K-Fold Cross-Validation for Yield Prediction\n",
    "# =============================================================================\n",
    "\n",
    "def kfold_evaluation(model_class, X, y, n_splits=5, epochs=50, device=DEVICE):\n",
    "    \"\"\"Perform K-Fold CV and return fold-wise metrics.\"\"\"\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_SEED)\n",
    "    fold_metrics = {'train_loss': [], 'val_loss': [], 'r2': [], 'mae': []}\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "        X_train_fold, X_val_fold = X[train_idx], X[val_idx]\n",
    "        y_train_fold, y_val_fold = y[train_idx], y[val_idx]\n",
    "        \n",
    "        # Convert to tensors\n",
    "        X_train_t = torch.FloatTensor(X_train_fold).to(device)\n",
    "        y_train_t = torch.FloatTensor(y_train_fold).reshape(-1, 1).to(device)\n",
    "        X_val_t = torch.FloatTensor(X_val_fold).to(device)\n",
    "        y_val_t = torch.FloatTensor(y_val_fold).reshape(-1, 1).to(device)\n",
    "        \n",
    "        # Initialize model\n",
    "        model = model_class(X.shape[1]).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        # Train\n",
    "        model.train()\n",
    "        for _ in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(X_train_t)\n",
    "            loss = criterion(pred, y_train_t)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Evaluate\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            train_pred = model(X_train_t).cpu().numpy()\n",
    "            val_pred = model(X_val_t).cpu().numpy()\n",
    "            \n",
    "        fold_metrics['train_loss'].append(mean_squared_error(y_train_fold, train_pred))\n",
    "        fold_metrics['val_loss'].append(mean_squared_error(y_val_fold, val_pred))\n",
    "        fold_metrics['r2'].append(r2_score(y_val_fold, val_pred))\n",
    "        fold_metrics['mae'].append(mean_absolute_error(y_val_fold, val_pred))\n",
    "    \n",
    "    return fold_metrics\n",
    "\n",
    "# Simple MLP for CV testing\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Prepare data for CV\n",
    "X_cv = X_clim_y_scaled if 'X_clim_y_scaled' in dir() else X_climate_scaled\n",
    "y_cv = y_yield_scaled if 'y_yield_scaled' in dir() else y_yield\n",
    "\n",
    "print(\"K-FOLD CROSS-VALIDATION: Yield Predictor\")\n",
    "print(\"=\" * 60)\n",
    "cv_results = kfold_evaluation(SimpleMLP, X_cv, y_cv, n_splits=5, epochs=100)\n",
    "\n",
    "for metric, values in cv_results.items():\n",
    "    print(f\"{metric:12}: Mean = {np.mean(values):.4f} Â± {np.std(values):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66129318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 11.2 Time Series Cross-Validation for Price LSTM\n",
    "# =============================================================================\n",
    "\n",
    "def timeseries_cv(X, Y, n_splits=3, window=WINDOW):\n",
    "    \"\"\"Time series cross-validation with expanding window.\"\"\"\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    ts_metrics = {'fold': [], 'train_size': [], 'test_size': [], 'rmse': [], 'mae': []}\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(tscv.split(X)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        Y_train, Y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        # Use simple linear regression as baseline\n",
    "        X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "        X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "        Y_train_flat = Y_train.reshape(Y_train.shape[0], -1)\n",
    "        Y_test_flat = Y_test.reshape(Y_test.shape[0], -1)\n",
    "        \n",
    "        lr = LinearRegression()\n",
    "        lr.fit(X_train_flat, Y_train_flat)\n",
    "        pred = lr.predict(X_test_flat)\n",
    "        \n",
    "        ts_metrics['fold'].append(fold + 1)\n",
    "        ts_metrics['train_size'].append(len(train_idx))\n",
    "        ts_metrics['test_size'].append(len(test_idx))\n",
    "        ts_metrics['rmse'].append(np.sqrt(mean_squared_error(Y_test_flat, pred)))\n",
    "        ts_metrics['mae'].append(mean_absolute_error(Y_test_flat, pred))\n",
    "    \n",
    "    return pd.DataFrame(ts_metrics)\n",
    "\n",
    "print(\"TIME SERIES CROSS-VALIDATION: Price Forecasting\")\n",
    "print(\"=\" * 60)\n",
    "ts_cv_results = timeseries_cv(X_rice, Y_rice, n_splits=4)\n",
    "print(ts_cv_results.to_string(index=False))\n",
    "print(f\"\\nMean RMSE: {ts_cv_results['rmse'].mean():.4f} Â± {ts_cv_results['rmse'].std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a5da44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 11.3 Model Performance Visualization\n",
    "# =============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Model Generalization Analysis', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 1. K-Fold CV Results\n",
    "ax = axes[0, 0]\n",
    "fold_nums = range(1, 6)\n",
    "ax.bar(fold_nums, cv_results['val_loss'], alpha=0.7, label='Validation Loss', color='#3498db')\n",
    "ax.axhline(np.mean(cv_results['val_loss']), color='red', linestyle='--', label=f\"Mean: {np.mean(cv_results['val_loss']):.4f}\")\n",
    "ax.fill_between(fold_nums, \n",
    "                np.mean(cv_results['val_loss']) - np.std(cv_results['val_loss']),\n",
    "                np.mean(cv_results['val_loss']) + np.std(cv_results['val_loss']),\n",
    "                alpha=0.2, color='red', label='Â±1 Std')\n",
    "ax.set_xlabel('Fold')\n",
    "ax.set_ylabel('MSE Loss')\n",
    "ax.set_title('K-Fold CV: Yield Predictor')\n",
    "ax.legend()\n",
    "ax.set_xticks(fold_nums)\n",
    "\n",
    "# 2. Time Series CV Results\n",
    "ax = axes[0, 1]\n",
    "x_ts = range(len(ts_cv_results))\n",
    "ax.plot(ts_cv_results['train_size'], ts_cv_results['rmse'], 'o-', label='RMSE vs Train Size', markersize=8)\n",
    "ax.set_xlabel('Training Set Size')\n",
    "ax.set_ylabel('RMSE')\n",
    "ax.set_title('Learning Curve: Price Forecasting')\n",
    "for i, (train, rmse) in enumerate(zip(ts_cv_results['train_size'], ts_cv_results['rmse'])):\n",
    "    ax.annotate(f'Fold {i+1}', (train, rmse), textcoords='offset points', xytext=(5, 5), fontsize=8)\n",
    "\n",
    "# 3. RÂ² Distribution\n",
    "ax = axes[1, 0]\n",
    "r2_values = cv_results['r2']\n",
    "ax.hist(r2_values, bins=5, edgecolor='black', alpha=0.7, color='#2ecc71')\n",
    "ax.axvline(np.mean(r2_values), color='red', linestyle='--', label=f\"Mean RÂ²: {np.mean(r2_values):.3f}\")\n",
    "ax.set_xlabel('RÂ² Score')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('RÂ² Score Distribution (K-Fold)')\n",
    "ax.legend()\n",
    "\n",
    "# 4. Train vs Validation Loss Comparison\n",
    "ax = axes[1, 1]\n",
    "width = 0.35\n",
    "x = np.arange(len(fold_nums))\n",
    "ax.bar(x - width/2, cv_results['train_loss'], width, label='Train', alpha=0.7, color='#3498db')\n",
    "ax.bar(x + width/2, cv_results['val_loss'], width, label='Validation', alpha=0.7, color='#e74c3c')\n",
    "ax.set_xlabel('Fold')\n",
    "ax.set_ylabel('MSE Loss')\n",
    "ax.set_title('Train vs Validation Loss')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([f'Fold {i}' for i in fold_nums])\n",
    "ax.legend()\n",
    "\n",
    "# Calculate generalization gap\n",
    "gen_gap = np.mean(np.array(cv_results['val_loss']) - np.array(cv_results['train_loss']))\n",
    "ax.text(0.95, 0.95, f'Generalization Gap: {gen_gap:.4f}', transform=ax.transAxes,\n",
    "        fontsize=9, ha='right', va='top', bbox=dict(boxstyle='round', facecolor='wheat'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nGENERALIZATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Yield Predictor - Mean Validation MSE: {np.mean(cv_results['val_loss']):.4f}\")\n",
    "print(f\"Yield Predictor - Mean RÂ²: {np.mean(cv_results['r2']):.4f}\")\n",
    "print(f\"Price LSTM Baseline - Mean RMSE: {ts_cv_results['rmse'].mean():.4f}\")\n",
    "print(f\"Generalization Gap: {gen_gap:.4f} {'(Low - Good)' if abs(gen_gap) < 0.1 else '(High - Overfitting risk)'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a83329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 11.4 Baseline Model Comparison\n",
    "# =============================================================================\n",
    "\n",
    "print(\"BASELINE MODEL COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Prepare data\n",
    "X_baseline = X_cv\n",
    "y_baseline = y_cv\n",
    "\n",
    "# Split\n",
    "X_train_b, X_test_b, y_train_b, y_test_b = train_test_split(\n",
    "    X_baseline, y_baseline, test_size=0.2, random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "# Models to compare\n",
    "baseline_results = []\n",
    "\n",
    "# 1. Linear Regression\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train_b, y_train_b)\n",
    "pred_lr = lr_model.predict(X_test_b)\n",
    "baseline_results.append({\n",
    "    'Model': 'Linear Regression',\n",
    "    'RÂ²': r2_score(y_test_b, pred_lr),\n",
    "    'RMSE': np.sqrt(mean_squared_error(y_test_b, pred_lr)),\n",
    "    'MAE': mean_absolute_error(y_test_b, pred_lr)\n",
    "})\n",
    "\n",
    "# 2. Random Forest\n",
    "rf = RandomForestRegressor(n_estimators=50, random_state=RANDOM_SEED, n_jobs=-1)\n",
    "rf.fit(X_train_b, y_train_b)\n",
    "pred_rf = rf.predict(X_test_b)\n",
    "baseline_results.append({\n",
    "    'Model': 'Random Forest',\n",
    "    'RÂ²': r2_score(y_test_b, pred_rf),\n",
    "    'RMSE': np.sqrt(mean_squared_error(y_test_b, pred_rf)),\n",
    "    'MAE': mean_absolute_error(y_test_b, pred_rf)\n",
    "})\n",
    "\n",
    "# 3. Neural Network (Simple MLP)\n",
    "class BaselineMLP(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "mlp_baseline = BaselineMLP(X_train_b.shape[1]).to(DEVICE)\n",
    "opt_mlp = torch.optim.Adam(mlp_baseline.parameters(), lr=0.01)\n",
    "crit_mlp = nn.MSELoss()\n",
    "\n",
    "X_train_t = torch.FloatTensor(X_train_b).to(DEVICE)\n",
    "y_train_t = torch.FloatTensor(y_train_b).reshape(-1, 1).to(DEVICE)\n",
    "X_test_t = torch.FloatTensor(X_test_b).to(DEVICE)\n",
    "\n",
    "mlp_baseline.train()\n",
    "for _ in range(100):\n",
    "    opt_mlp.zero_grad()\n",
    "    out = mlp_baseline(X_train_t)\n",
    "    loss = crit_mlp(out, y_train_t)\n",
    "    loss.backward()\n",
    "    opt_mlp.step()\n",
    "\n",
    "mlp_baseline.eval()\n",
    "with torch.no_grad():\n",
    "    pred_mlp = mlp_baseline(X_test_t).cpu().numpy().flatten()\n",
    "\n",
    "baseline_results.append({\n",
    "    'Model': 'MLP Neural Network',\n",
    "    'RÂ²': r2_score(y_test_b, pred_mlp),\n",
    "    'RMSE': np.sqrt(mean_squared_error(y_test_b, pred_mlp)),\n",
    "    'MAE': mean_absolute_error(y_test_b, pred_mlp)\n",
    "})\n",
    "\n",
    "# Display results\n",
    "baseline_df = pd.DataFrame(baseline_results)\n",
    "print(baseline_df.to_string(index=False))\n",
    "\n",
    "# Visualize comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "x = np.arange(len(baseline_df))\n",
    "width = 0.25\n",
    "\n",
    "ax.bar(x - width, baseline_df['RÂ²'], width, label='RÂ²', alpha=0.8)\n",
    "ax.bar(x, baseline_df['RMSE'], width, label='RMSE', alpha=0.8)\n",
    "ax.bar(x + width, baseline_df['MAE'], width, label='MAE', alpha=0.8)\n",
    "\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Model Performance Comparison')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(baseline_df['Model'])\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a329e0",
   "metadata": {},
   "source": [
    "## 13. Multi-Crop Price Forecasting LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0de483e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiCropPriceLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden=64, layers=2, dropout=0.2, horizon=7):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden, layers, batch_first=True, dropout=dropout if layers > 1 else 0)\n",
    "        self.fc = nn.Sequential(nn.Linear(hidden, 32), nn.ReLU(), nn.Linear(32, horizon))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        return self.fc(out[:, -1, :])\n",
    "\n",
    "# Use existing data from earlier cell\n",
    "df_rice_clean = df_rice.copy()\n",
    "price_feature_scaler = MinMaxScaler()\n",
    "price_scaled = price_feature_scaler.fit_transform(features)\n",
    "\n",
    "def make_seqs(data, target_col, window, horizon):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(data) - window - horizon + 1):\n",
    "        X.append(data[i:i+window])\n",
    "        Y.append(data[i+window:i+window+horizon, target_col])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "X_rice, Y_rice = make_seqs(price_scaled, 0, WINDOW, HORIZON)\n",
    "split_rice = int(len(X_rice) * 0.8)\n",
    "\n",
    "X_rice_train_t = torch.from_numpy(X_rice[:split_rice]).float().to(DEVICE)\n",
    "Y_rice_train_t = torch.from_numpy(Y_rice[:split_rice]).float().to(DEVICE)\n",
    "X_rice_test_t = torch.from_numpy(X_rice[split_rice:]).float().to(DEVICE)\n",
    "Y_rice_test_t = torch.from_numpy(Y_rice[split_rice:]).float().to(DEVICE)\n",
    "\n",
    "print(f\"Rice forecast: X={X_rice.shape}, Train={len(X_rice_train_t)}, Test={len(X_rice_test_t)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa32f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train price forecaster\n",
    "rice_price_model = MultiCropPriceLSTM(len(feature_cols), horizon=HORIZON).to(DEVICE)\n",
    "opt_price = torch.optim.Adam(rice_price_model.parameters(), lr=1e-3)\n",
    "crit_price = nn.MSELoss()\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    rice_price_model.train()\n",
    "    idx = np.random.permutation(len(X_rice_train_t))\n",
    "    \n",
    "    for start in range(0, len(idx), 16):\n",
    "        batch = idx[start:start+16]\n",
    "        opt_price.zero_grad()\n",
    "        loss = crit_price(rice_price_model(X_rice_train_t[batch]), Y_rice_train_t[batch])\n",
    "        loss.backward()\n",
    "        opt_price.step()\n",
    "    \n",
    "    if epoch % 20 == 0:\n",
    "        rice_price_model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_loss = crit_price(rice_price_model(X_rice_test_t), Y_rice_test_t)\n",
    "        print(f'Epoch {epoch}: Train={loss.item():.5f}, Test={test_loss:.5f}')\n",
    "\n",
    "# Evaluate\n",
    "rice_price_model.eval()\n",
    "with torch.no_grad():\n",
    "    test_pred = rice_price_model(X_rice_test_t).cpu().numpy()\n",
    "    test_actual = Y_rice_test_t.cpu().numpy()\n",
    "\n",
    "def inv_price(x):\n",
    "    dummy = np.zeros((len(x), len(feature_cols)))\n",
    "    dummy[:, 0] = x\n",
    "    return price_feature_scaler.inverse_transform(dummy)[:, 0]\n",
    "\n",
    "pred_1m = inv_price(test_pred[:, 0])\n",
    "actual_1m = inv_price(test_actual[:, 0])\n",
    "rmse_price = np.sqrt(mean_squared_error(actual_1m, pred_1m))\n",
    "print(f'\\nPrice Forecast (1-month) - RMSE: {rmse_price:.2f} LKR')\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(actual_1m, label='Actual', alpha=0.8)\n",
    "plt.plot(pred_1m, label='Predicted', alpha=0.8)\n",
    "plt.xlabel('Sample'); plt.ylabel('Price (LKR)'); plt.title('Rice Price Forecast')\n",
    "plt.legend(); plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5490323",
   "metadata": {},
   "source": [
    "## 14. Model Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238b0bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, json\n",
    "\n",
    "MODELS_DIR = Path('../app/models')\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save PyTorch models\n",
    "torch.save({'model_state_dict': crop_model.state_dict(), 'n_regions': n_regions, 'n_crops': n_crops, 'embedding_dim': 16}, MODELS_DIR / 'crop_recommender.pt')\n",
    "torch.save({'model_state_dict': yield_model.state_dict(), 'n_regions': n_regions, 'n_crops': n_crops, 'embedding_dim': 8}, MODELS_DIR / 'yield_predictor.pt')\n",
    "torch.save({'model_state_dict': rice_price_model.state_dict(), 'input_size': len(feature_cols), 'hidden_size': 64, 'num_layers': 2, 'horizon': HORIZON, 'window': WINDOW}, MODELS_DIR / 'rice_price_lstm.pt')\n",
    "\n",
    "print(\"Saved: crop_recommender.pt, yield_predictor.pt, rice_price_lstm.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e947fdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save encoders and scalers\n",
    "for name, obj in [('region_encoder', region_enc), ('crop_encoder', crop_enc), \n",
    "                  ('climate_scaler', climate_scaler), ('yield_scaler', yield_scaler), \n",
    "                  ('price_feature_scaler', price_feature_scaler)]:\n",
    "    with open(MODELS_DIR / f'{name}.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "metadata = {\n",
    "    'regions': region_enc.classes_.tolist(),\n",
    "    'crops': crop_enc.classes_.tolist(),\n",
    "    'climate_features': ['temperature', 'rainfall', 'humidity', 'month'],\n",
    "    'price_features': feature_cols,\n",
    "    'forecast_window': WINDOW,\n",
    "    'forecast_horizon': HORIZON,\n",
    "    'created_at': pd.Timestamp.now().isoformat()\n",
    "}\n",
    "with open(MODELS_DIR / 'metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"All artifacts saved to: {MODELS_DIR.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410efb75",
   "metadata": {},
   "source": [
    "## 15. Service Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758aaaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_crop(region, temp, rain, hum, month, top_k=5):\n",
    "    crop_model.eval()\n",
    "    try:\n",
    "        r_idx = region_enc.transform([region])[0]\n",
    "    except ValueError:\n",
    "        return []\n",
    "    \n",
    "    climate = climate_scaler.transform([[temp, rain, hum, month]])\n",
    "    with torch.no_grad():\n",
    "        logits = crop_model(\n",
    "            torch.tensor([r_idx]).long().to(DEVICE),\n",
    "            torch.from_numpy(climate).float().to(DEVICE)\n",
    "        )\n",
    "        probs = torch.softmax(logits, dim=1)[0].cpu().numpy()\n",
    "    \n",
    "    top_idx = np.argsort(probs)[-top_k:][::-1]\n",
    "    return [(crop_enc.classes_[i], float(probs[i])) for i in top_idx]\n",
    "\n",
    "class ACAOService:\n",
    "    def __init__(self):\n",
    "        self.device = DEVICE\n",
    "        self.crop_model = crop_model\n",
    "        self.yield_model = yield_model\n",
    "        self.price_model = rice_price_model\n",
    "        self.region_enc = region_enc\n",
    "        self.crop_enc = crop_enc\n",
    "        self.climate_scaler = climate_scaler\n",
    "        self.yield_scaler = yield_scaler\n",
    "        self.price_scaler = price_feature_scaler\n",
    "        self.regions = region_enc.classes_.tolist()\n",
    "        self.crops = crop_enc.classes_.tolist()\n",
    "    \n",
    "    def get_crop_recommendations(self, region, temp, rain, hum, month, top_k=5):\n",
    "        return recommend_crop(region, temp, rain, hum, month, top_k)\n",
    "    \n",
    "    def predict_yield(self, region, crop, temp, rain, hum, month):\n",
    "        return predict_yield_for_crop(region, crop, temp, rain, hum, month)\n",
    "    \n",
    "    def optimize_area(self, region, crops, total_area, temp, rain, hum, month, min_div=0.05):\n",
    "        opt = AreaOptimizer(crops=crops, region=region, total_area=total_area)\n",
    "        return opt.optimize(temp, rain, hum, month, min_div)\n",
    "    \n",
    "    def get_regions(self): return self.regions\n",
    "    def get_crops(self): return self.crops\n",
    "    \n",
    "    def get_regional_summary(self, region):\n",
    "        if region in regional.index:\n",
    "            d = regional.loc[region].to_dict()\n",
    "            return {'region': region, 'avg_temp': d.get('temperature', 0), \n",
    "                    'avg_rain': d.get('rainfall', 0), 'avg_yield': d.get('yield_impact_score', 0)}\n",
    "        return {'region': region, 'error': 'Not found'}\n",
    "\n",
    "acao_service = ACAOService()\n",
    "print(f\"ACA-O Service: {len(acao_service.regions)} regions, {len(acao_service.crops)} crops\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317961d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo workflow\n",
    "demo = {'region': 'Anuradhapura', 'month': 10, 'temp': 29, 'rain': 180, 'hum': 78}\n",
    "print(f\"Demo: {demo['region']}, Maha Season\\nClimate: {demo['temp']}C, {demo['rain']}mm, {demo['hum']}%\")\n",
    "\n",
    "# Regional summary\n",
    "summary = acao_service.get_regional_summary(demo['region'])\n",
    "print(f\"\\nHistorical: temp={summary.get('avg_temp', 'N/A')}, rain={summary.get('avg_rain', 'N/A')}\")\n",
    "\n",
    "# Crop recommendations\n",
    "print(\"\\nTop 5 Crops:\")\n",
    "recs = acao_service.get_crop_recommendations(demo['region'], demo['temp'], demo['rain'], demo['hum'], demo['month'], 5)\n",
    "for i, (c, conf) in enumerate(recs, 1):\n",
    "    print(f\"  {i}. {c}: {conf:.1%}\")\n",
    "\n",
    "# Yield predictions\n",
    "print(\"\\nYield Predictions:\")\n",
    "top_crops = [c for c, _ in recs[:5]]\n",
    "for c in top_crops:\n",
    "    y = acao_service.predict_yield(demo['region'], c, demo['temp'], demo['rain'], demo['hum'], demo['month'])\n",
    "    print(f\"  {c}: {y:.2f}\")\n",
    "\n",
    "# Area optimization\n",
    "print(\"\\nOptimal Allocation (100 ha):\")\n",
    "opt = acao_service.optimize_area(demo['region'], top_crops, 100, demo['temp'], demo['rain'], demo['hum'], demo['month'])\n",
    "for c, a, p in zip(opt['crops'], opt['allocations_ha'], opt['allocation_pct']):\n",
    "    print(f\"  {c}: {a:.1f}ha ({p:.1f}%)\")\n",
    "print(f\"\\nExpected Revenue: LKR {opt['total_expected_revenue']:,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d83436",
   "metadata": {},
   "source": [
    "## 16. Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94ed0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 10))\n",
    "\n",
    "ax1 = fig.add_subplot(2, 3, 1)\n",
    "top_reg = regional.nlargest(10, 'yield_impact_score')\n",
    "ax1.barh(top_reg.index, top_reg['yield_impact_score'], color='forestgreen')\n",
    "ax1.set_xlabel('Yield Score'); ax1.set_title('Top Regions by Yield'); ax1.invert_yaxis()\n",
    "\n",
    "ax2 = fig.add_subplot(2, 3, 2)\n",
    "sc = ax2.scatter(regional['temperature'], regional['yield_impact_score'],\n",
    "                 c=regional['rainfall'], cmap='Blues', s=80, alpha=0.7)\n",
    "ax2.set_xlabel('Temp (C)'); ax2.set_ylabel('Yield Score'); ax2.set_title('Climate vs Yield')\n",
    "plt.colorbar(sc, ax=ax2, label='Rain (mm)')\n",
    "\n",
    "ax3 = fig.add_subplot(2, 3, 3)\n",
    "crops_dist = df_hist['vegetable_type'].value_counts().head(8)\n",
    "ax3.pie(crops_dist.values, labels=crops_dist.index, autopct='%1.0f%%')\n",
    "ax3.set_title('Crop Distribution')\n",
    "\n",
    "ax4 = fig.add_subplot(2, 3, 4)\n",
    "df_hist['month'] = df_hist['date'].dt.month\n",
    "monthly = df_hist.groupby('month')['vegetable_price'].mean()\n",
    "ax4.plot(monthly.index, monthly.values, 'o-', lw=2)\n",
    "ax4.set_xlabel('Month'); ax4.set_ylabel('Price (LKR)'); ax4.set_title('Seasonal Prices')\n",
    "\n",
    "ax5 = fig.add_subplot(2, 3, 5)\n",
    "ax5.plot(df_rice['date'], df_rice['price'], lw=0.8)\n",
    "ax5.set_xlabel('Date'); ax5.set_ylabel('Price (LKR)'); ax5.set_title('Rice Price History')\n",
    "\n",
    "ax6 = fig.add_subplot(2, 3, 6)\n",
    "models = ['Crop Rec', 'Yield', 'Price']\n",
    "scores = [acc.item() * 100, max(0, 100 - 0.1525 * 100), max(0, 100 - rmse_price / 10)]\n",
    "ax6.bar(models, scores, color=['#2ecc71', '#3498db', '#9b59b6'])\n",
    "ax6.set_ylabel('Score'); ax6.set_title('Model Performance'); ax6.set_ylim(0, 100)\n",
    "\n",
    "plt.suptitle('ACA-O Sri Lanka Dashboard', fontsize=14, fontweight='bold', y=1.01)\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3de5ac",
   "metadata": {},
   "source": [
    "## 17. Conclusion & Summary\n",
    "\n",
    "### System Overview\n",
    "The ACA-O (Adaptive Crop & Area Optimization) system integrates multiple ML models for agricultural decision support in Sri Lanka.\n",
    "\n",
    "### Dataset Summary\n",
    "| Dataset | Records | Purpose |\n",
    "|---------|---------|---------|\n",
    "| Paddy Statistics | 28 | Regional crop-yield baselines |\n",
    "| Economy (Rice) | 61 | Economic indicators, production trends |\n",
    "| Vegetable Prices | 39 | Current market pricing |\n",
    "| Historical Vegetables | 130,000 | Price pattern learning |\n",
    "| Rice Time Series | 324 | Monthly price sequences |\n",
    "| Climate Data | 1,800 | Weather-yield correlations |\n",
    "\n",
    "### Model Performance\n",
    "| Model | Architecture | Metric | Value |\n",
    "|-------|--------------|--------|-------|\n",
    "| CropRecommender | Region Embedding + MLP | Accuracy | ~36% |\n",
    "| YieldPredictor | Dual Embedding + MLP | RMSE | ~0.48 |\n",
    "| PriceLSTM | LSTM (2 layers) | RMSE | ~0.06 |\n",
    "| AreaOptimizer | Linear Programming | Revenue | Optimized |\n",
    "\n",
    "### Key Insights\n",
    "1. **Data Quality**: Historical vegetable dataset provides most training value\n",
    "2. **Regional Patterns**: 25 Sri Lankan districts show distinct climate-yield correlations\n",
    "3. **Seasonality**: Maha and Yala seasons significantly impact optimal crop selection\n",
    "4. **Price Volatility**: LSTM captures multi-horizon price patterns effectively\n",
    "\n",
    "### Next Steps\n",
    "- Integrate real-time weather API data\n",
    "- Add more official government agricultural statistics\n",
    "- Implement ensemble methods for improved accuracy\n",
    "- Deploy as microservice in the irrigation system"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
